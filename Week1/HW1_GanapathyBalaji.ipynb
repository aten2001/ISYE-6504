{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 1 - Ganapathy Raaman Balaji"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.1\n",
    "\n",
    "##### Describe a situation or problem from your job, everyday life, current events, etc., for which a classification model would be appropriate. List some (up to 5) predictors that you might use.\n",
    "\n",
    "````\n",
    "I am a Performance Analytics Engineer at Caterpillar. Performing exploratory analysis one high frequency engine and machine is my daily job. One of the projects I worked on was to classify gensets into different applications, based on the type of operation. I use Python for analysis, and used the Scikit Learn package in Python. I performed Regression Analysis on the following predictors to identify and classify the type of application based on a window of time of engine operation after ignition on.\n",
    "\n",
    "Some of the factors are:\n",
    "\n",
    "1. Engine RPM profile in this window (includes factors such as slope of change in a given time)\n",
    "2. How much the engine is loaded in this window\n",
    "3. Duration the genset engine was running\n",
    "4. Generator Power Output\n",
    "5. Fuel Consumption\n",
    "\n",
    "````"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.2 (a):\n",
    "\n",
    "##### Overall Best Solution: rbfdot seems to provide an accuracy of 99.54128% for cost value = 10000\n",
    "```\n",
    "Best vanilladot solution: Accuracy of 86.39144% for cost value = 1\n",
    "Best anovadot solution: Accuracy of 90.82569% for cost value = 10000\n",
    "Best polydot solution: Accuracy of 86.39144% for cost value = 1\n",
    "\n",
    "We see different results if we split the data to train and test datasets. I have explained it in the following section.\n",
    "```\n",
    "##### I have also worked on this problem by splitting the data set to train-test (80-20) split. \n",
    "\n",
    "#### Stepwise explanation to the problem approach: \n",
    "```\n",
    "In this problem, I have created vectors of different kernels and cost values. I am looping through each kernel list, and a nested loop to iterate over the different cost values. For each kernel and each cost value, I am appending the model accuracy, weights, bias, name of the kernel and the corresponding cost value to its own empty vectors. After this step, I am writing these vectors to a dataframe. Now, the weights are a one dimensional array of a1,a2...,am for each of the cost value and kernel. I am reshaping this array so the first row would contain columns of a1,...am for each C value. The data frame is also sorted by descending order of Model Accuracy.\n",
    "```\n",
    "#### The equation of the model, based on the highest model accuracy is:\n",
    "```\n",
    "Response, y = (-56.5077762 x A1) + (-90.32754705 x A2) + (-184.3305137 x A3) + (84.07522492 x A8) + (52.8714348 x A9) + (-108.9002671 x A10) + (94.62057917 x A11) + (-42.56268646 x A12) + (-100.3447438 x A14) + (94.97922712 x A15) - 0.88504516\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th></th><th scope=col>Kernel</th><th scope=col>Cost Value</th><th scope=col>Model Accuracy</th><th scope=col>a0</th><th scope=col>V1</th><th scope=col>V2</th><th scope=col>V3</th><th scope=col>V4</th><th scope=col>V5</th><th scope=col>V6</th><th scope=col>V7</th><th scope=col>V8</th><th scope=col>V9</th><th scope=col>V10</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>5</th><td>rbfdot         </td><td>10000.000      </td><td>99.23547       </td><td> -0.63824146   </td><td>-76.93709192106</td><td> -73.3619155041</td><td>-148.4894056079</td><td>126.5889917002 </td><td>68.3144152     </td><td>-135.9618828193</td><td>119.3898440611 </td><td>-53.2487429501 </td><td>-87.7497435946 </td><td>110.6058081105 </td></tr>\n",
       "\t<tr><th scope=row>4</th><td>rbfdot         </td><td> 1000.000      </td><td>98.01223       </td><td> -0.70466937   </td><td>-58.53155010645</td><td> -12.6283069575</td><td> -40.4821556669</td><td>136.9102651836 </td><td>83.0580193     </td><td> -95.3351838228</td><td>111.6392718642 </td><td>-70.8968195313 </td><td>-81.2003783239 </td><td>100.8872134471 </td></tr>\n",
       "\t<tr><th scope=row>3</th><td>rbfdot         </td><td>  100.000      </td><td>95.25994       </td><td> -0.78805310   </td><td>-19.19664569632</td><td> -37.2038168337</td><td>  -8.6463593605</td><td> 56.0607421094 </td><td>50.0993321     </td><td> -24.5243903993</td><td> 15.6733704383 </td><td>-24.1202777780 </td><td>-58.0518009098 </td><td> 51.6569482272 </td></tr>\n",
       "\t<tr><th scope=row>2</th><td>rbfdot         </td><td>   10.000      </td><td>91.43731       </td><td> -0.44146303   </td><td> -3.01817583732</td><td> -17.8291518521</td><td>   3.7185983605</td><td> 26.3513848229 </td><td>31.4942801     </td><td> -12.4592419207</td><td> 15.6013370037 </td><td> -9.6850171040 </td><td>-32.8608467791 </td><td> 36.7662448302 </td></tr>\n",
       "\t<tr><th scope=row>14</th><td>anovadot       </td><td>10000.000      </td><td>90.82569       </td><td>-21.79686445   </td><td>  0.11124018539</td><td>-104.1006008680</td><td>-199.0308606894</td><td> 71.2938356661 </td><td> 2.7999202     </td><td>  -1.9210920329</td><td> 67.0021356644 </td><td> -0.0514912609 </td><td>-81.3849955504 </td><td> 18.9760701171 </td></tr>\n",
       "\t<tr><th scope=row>11</th><td>anovadot       </td><td>  100.000      </td><td>90.67278       </td><td> -1.17407433   </td><td>  0.01883723961</td><td> -22.4979970645</td><td> -28.0511250529</td><td> -2.3583891057 </td><td> 2.5358136     </td><td>  -1.1223279448</td><td> -3.1148253872 </td><td> -0.0459050276 </td><td>-16.4453742797 </td><td> 14.8248172163 </td></tr>\n",
       "\t<tr><th scope=row>13</th><td>anovadot       </td><td> 1000.000      </td><td>90.67278       </td><td> -8.83058576   </td><td>  0.13349017779</td><td> -29.4473951794</td><td> -69.1024744447</td><td>-21.9604353805 </td><td> 2.6573468     </td><td>  -1.6872468945</td><td> -5.8671108816 </td><td> -0.0109795499 </td><td>-42.5112125620 </td><td>  9.8814263541 </td></tr>\n",
       "\t<tr><th scope=row>10</th><td>anovadot       </td><td>   10.000      </td><td>87.30887       </td><td> -0.02852669   </td><td>  0.01108490367</td><td>  -8.1314613538</td><td> -10.5797616103</td><td>  3.7790032035 </td><td> 2.2218562     </td><td>  -0.3590986015</td><td>  4.4898660412 </td><td>  0.0021407211 </td><td> -7.8947856724 </td><td> 18.9730392423 </td></tr>\n",
       "\t<tr><th scope=row>32</th><td>rbfdot         </td><td>    1.000      </td><td>87.00306       </td><td> -0.42067564   </td><td>  0.58153076023</td><td>  -1.6867985593</td><td>   3.7221663922</td><td> 12.6751302574 </td><td>30.1475563     </td><td>  -6.2640926105</td><td> 17.9315722616 </td><td> -4.6302198083 </td><td>-15.7188968881 </td><td> 26.2287199513 </td></tr>\n",
       "\t<tr><th scope=row>1</th><td>vanilladot     </td><td>    1.000      </td><td>86.39144       </td><td> -0.08148382   </td><td> -0.00110266416</td><td>  -0.0008980539</td><td>  -0.0016074557</td><td>  0.0029041700 </td><td> 1.0047363     </td><td>  -0.0029852110</td><td> -0.0002035179 </td><td> -0.0005504803 </td><td> -0.0012519187 </td><td>  0.1064404601 </td></tr>\n",
       "\t<tr><th scope=row>9</th><td>anovadot       </td><td>    1.000      </td><td>86.39144       </td><td> -0.37489521   </td><td>  0.00190220241</td><td>  -1.5386990249</td><td>  -0.9004786939</td><td>  0.6984353224 </td><td> 2.0504384     </td><td>  -0.0270396197</td><td>  0.7678055909 </td><td>  0.0011966941 </td><td> -2.7190432338 </td><td> 18.6897396524 </td></tr>\n",
       "\t<tr><th scope=row>12</th><td>vanilladot     </td><td>   10.000      </td><td>86.39144       </td><td> -0.08157559   </td><td> -0.00090336713</td><td>  -0.0007891036</td><td>  -0.0016972133</td><td>  0.0026113629 </td><td> 1.0050221     </td><td>  -0.0028363016</td><td> -0.0001569285 </td><td> -0.0003925964 </td><td> -0.0012784443 </td><td>  0.1064387167 </td></tr>\n",
       "\t<tr><th scope=row>18</th><td>polydot        </td><td>    1.000      </td><td>86.39144       </td><td> -0.08148471   </td><td> -0.00117790293</td><td>  -0.0007585829</td><td>  -0.0015830018</td><td>  0.0030741611 </td><td> 1.0045976     </td><td>  -0.0028875480</td><td>  0.0001266113 </td><td> -0.0006759177 </td><td> -0.0013468793 </td><td>  0.1064496302 </td></tr>\n",
       "\t<tr><th scope=row>19</th><td>polydot        </td><td>   10.000      </td><td>86.39144       </td><td> -0.08154590   </td><td> -0.00096471814</td><td>  -0.0010953376</td><td>  -0.0015706841</td><td>  0.0026559397 </td><td> 1.0050172     </td><td>  -0.0028674084</td><td> -0.0002663424 </td><td> -0.0005284851 </td><td> -0.0013955524 </td><td>  0.1064078260 </td></tr>\n",
       "\t<tr><th scope=row>20</th><td>polydot        </td><td>  100.000      </td><td>86.39144       </td><td> -0.08157716   </td><td> -0.00109297047</td><td>  -0.0012425741</td><td>  -0.0015628157</td><td>  0.0027739329 </td><td> 1.0051781     </td><td>  -0.0026901076</td><td> -0.0001935512 </td><td> -0.0005270357 </td><td> -0.0014583698 </td><td>  0.1063997443 </td></tr>\n",
       "\t<tr><th scope=row>23</th><td>vanilladot     </td><td>  100.000      </td><td>86.39144       </td><td> -0.08158492   </td><td> -0.00100653481</td><td>  -0.0011729048</td><td>  -0.0016261967</td><td>  0.0030064203 </td><td> 1.0049406     </td><td>  -0.0028259432</td><td>  0.0002600295 </td><td> -0.0005349551 </td><td> -0.0012283758 </td><td>  0.1063633995 </td></tr>\n",
       "\t<tr><th scope=row>24</th><td>polydot        </td><td>    0.100      </td><td>86.39144       </td><td> -0.08165418   </td><td> -0.00121245690</td><td>  -0.0006070979</td><td>  -0.0013956063</td><td>  0.0033049356 </td><td> 1.0040211     </td><td>  -0.0031961704</td><td>  0.0004545954 </td><td> -0.0003748261 </td><td> -0.0012915663 </td><td>  0.1064276327 </td></tr>\n",
       "\t<tr><th scope=row>25</th><td>polydot        </td><td>    0.010      </td><td>86.39144       </td><td> -0.08198853   </td><td> -0.00015007085</td><td>  -0.0014818363</td><td>   0.0014082877</td><td>  0.0072863924 </td><td> 0.9916470     </td><td>  -0.0044661326</td><td>  0.0071482901 </td><td> -0.0005468252 </td><td> -0.0016930686 </td><td>  0.1054824351 </td></tr>\n",
       "\t<tr><th scope=row>29</th><td>vanilladot     </td><td>    0.100      </td><td>86.39144       </td><td> -0.08155226   </td><td> -0.00116089805</td><td>  -0.0006366002</td><td>  -0.0015209679</td><td>  0.0032020638 </td><td> 1.0041339     </td><td>  -0.0033773669</td><td>  0.0002428616 </td><td> -0.0004747021 </td><td> -0.0011931900 </td><td>  0.1064450527 </td></tr>\n",
       "\t<tr><th scope=row>30</th><td>vanilladot     </td><td>    0.010      </td><td>86.39144       </td><td> -0.08198854   </td><td> -0.00015007376</td><td>  -0.0014818294</td><td>   0.0014083130</td><td>  0.0072863886 </td><td> 0.9916470     </td><td>  -0.0044661236</td><td>  0.0071482899 </td><td> -0.0005468386 </td><td> -0.0016930578 </td><td>  0.1054824270 </td></tr>\n",
       "\t<tr><th scope=row>15</th><td>anovadot       </td><td>    0.100      </td><td>86.23853       </td><td> -0.03920849   </td><td> -0.00001012938</td><td>  -0.1554799502</td><td>  -0.0846437302</td><td>  0.0708553124 </td><td> 2.0403653     </td><td>  -0.0018408626</td><td>  0.0804161637 </td><td>  0.0002612883 </td><td> -0.2748137619 </td><td>  1.8664937362 </td></tr>\n",
       "\t<tr><th scope=row>16</th><td>anovadot       </td><td>    0.010      </td><td>86.23853       </td><td> -0.14978982   </td><td> -0.00795464422</td><td>   0.0236412738</td><td>   0.0247243307</td><td>  0.2229233314 </td><td> 1.8114396     </td><td>  -0.0936787837</td><td>  0.2969301519 </td><td>  0.0071041248 </td><td> -0.1641909004 </td><td>  0.5162752844 </td></tr>\n",
       "\t<tr><th scope=row>21</th><td>polydot        </td><td> 1000.000      </td><td>86.23853       </td><td> -0.07044332   </td><td> -0.00022971942</td><td>  -0.0007721707</td><td>   0.0003367117</td><td>  0.0004066713 </td><td> 0.9983458     </td><td>  -0.0000170658</td><td>  0.0007563601 </td><td>  0.0002901689 </td><td>  0.0004152892 </td><td>  0.0006923412 </td></tr>\n",
       "\t<tr><th scope=row>22</th><td>polydot        </td><td>10000.000      </td><td>86.23853       </td><td> -0.07188214   </td><td> -0.00093819800</td><td>   0.0026823497</td><td>   0.0082815894</td><td>  0.0052093978 </td><td> 1.0092086     </td><td>  -0.0055107293</td><td>  0.0086526116 </td><td> -0.0010171646 </td><td> -0.0030672862 </td><td>  0.0123035355 </td></tr>\n",
       "\t<tr><th scope=row>27</th><td>vanilladot     </td><td> 1000.000      </td><td>86.23853       </td><td> -0.07017871   </td><td> -0.00021491854</td><td>   0.0007097786</td><td>   0.0011645166</td><td>  0.0005673024 </td><td> 0.9987192     </td><td>  -0.0005037912</td><td>  0.0007155434 </td><td> -0.0009130079 </td><td>  0.0007969700 </td><td>  0.0010062350 </td></tr>\n",
       "\t<tr><th scope=row>28</th><td>vanilladot     </td><td>10000.000      </td><td>86.23853       </td><td> -0.07046104   </td><td>  0.00089361671</td><td>   0.0016125725</td><td>  -0.0003415921</td><td>  0.0042114213 </td><td> 1.0014528     </td><td>   0.0023573637</td><td>  0.0052757867 </td><td>  0.0002803662 </td><td>  0.0025097815 </td><td>  0.0035920442 </td></tr>\n",
       "\t<tr><th scope=row>6</th><td>rbfdot         </td><td>    0.100      </td><td>85.93272       </td><td> -0.49895859   </td><td>  0.44800542306</td><td>   2.6930386158</td><td>   2.9894344888</td><td>  7.5262391844 </td><td>18.2014277     </td><td>  -4.3052704151</td><td>  7.2487672728 </td><td> -0.8599054048 </td><td> -2.7261636972 </td><td>  7.2245209456 </td></tr>\n",
       "\t<tr><th scope=row>26</th><td>polydot        </td><td>    0.001      </td><td>83.79205       </td><td>  0.22261555   </td><td> -0.00215977831</td><td>   0.0323381698</td><td>   0.0466124485</td><td>  0.1112231618 </td><td> 0.3753053     </td><td>  -0.2020260812</td><td>  0.1695608466 </td><td> -0.0049235007 </td><td> -0.0252102660 </td><td>  0.0811897661 </td></tr>\n",
       "\t<tr><th scope=row>31</th><td>vanilladot     </td><td>    0.001      </td><td>83.79205       </td><td>  0.22261554   </td><td> -0.00215977831</td><td>   0.0323381696</td><td>   0.0466124485</td><td>  0.1112231617 </td><td> 0.3753053     </td><td>  -0.2020260811</td><td>  0.1695608466 </td><td> -0.0049235009 </td><td> -0.0252102660 </td><td>  0.0811897661 </td></tr>\n",
       "\t<tr><th scope=row>17</th><td>anovadot       </td><td>    0.001      </td><td>58.86850       </td><td>  0.40814596   </td><td> -0.00495820271</td><td>   0.0732439801</td><td>   0.0901967366</td><td>  0.1794091425 </td><td> 0.4127070     </td><td>  -0.2396174520</td><td>  0.2332018560 </td><td> -0.0080173465 </td><td> -0.0483230716 </td><td>  0.1017114470 </td></tr>\n",
       "\t<tr><th scope=row>7</th><td>rbfdot         </td><td>    0.010      </td><td>56.72783       </td><td>  0.37843714   </td><td>  0.19438004769</td><td>   0.8585445702</td><td>   0.9615202459</td><td>  1.8307180198 </td><td> 4.1270703     </td><td>  -2.3961745198</td><td>  2.3320185596 </td><td> -0.1002168311 </td><td> -0.5739808530 </td><td>  1.0159886099 </td></tr>\n",
       "\t<tr><th scope=row>8</th><td>rbfdot         </td><td>    0.001      </td><td>54.74006       </td><td>  0.94002520   </td><td>  0.01943800477</td><td>   0.0867650055</td><td>   0.0980760604</td><td>  0.1828862979 </td><td> 0.4127070     </td><td>  -0.2396174520</td><td>  0.2332018560 </td><td> -0.0120260197 </td><td> -0.0574456150 </td><td>  0.1015973370 </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|llllllllllllll}\n",
       "  & Kernel & Cost Value & Model Accuracy & a0 & V1 & V2 & V3 & V4 & V5 & V6 & V7 & V8 & V9 & V10\\\\\n",
       "\\hline\n",
       "\t5 & rbfdot          & 10000.000       & 99.23547        &  -0.63824146    & -76.93709192106 &  -73.3619155041 & -148.4894056079 & 126.5889917002  & 68.3144152      & -135.9618828193 & 119.3898440611  & -53.2487429501  & -87.7497435946  & 110.6058081105 \\\\\n",
       "\t4 & rbfdot          &  1000.000       & 98.01223        &  -0.70466937    & -58.53155010645 &  -12.6283069575 &  -40.4821556669 & 136.9102651836  & 83.0580193      &  -95.3351838228 & 111.6392718642  & -70.8968195313  & -81.2003783239  & 100.8872134471 \\\\\n",
       "\t3 & rbfdot          &   100.000       & 95.25994        &  -0.78805310    & -19.19664569632 &  -37.2038168337 &   -8.6463593605 &  56.0607421094  & 50.0993321      &  -24.5243903993 &  15.6733704383  & -24.1202777780  & -58.0518009098  &  51.6569482272 \\\\\n",
       "\t2 & rbfdot          &    10.000       & 91.43731        &  -0.44146303    &  -3.01817583732 &  -17.8291518521 &    3.7185983605 &  26.3513848229  & 31.4942801      &  -12.4592419207 &  15.6013370037  &  -9.6850171040  & -32.8608467791  &  36.7662448302 \\\\\n",
       "\t14 & anovadot        & 10000.000       & 90.82569        & -21.79686445    &   0.11124018539 & -104.1006008680 & -199.0308606894 &  71.2938356661  &  2.7999202      &   -1.9210920329 &  67.0021356644  &  -0.0514912609  & -81.3849955504  &  18.9760701171 \\\\\n",
       "\t11 & anovadot        &   100.000       & 90.67278        &  -1.17407433    &   0.01883723961 &  -22.4979970645 &  -28.0511250529 &  -2.3583891057  &  2.5358136      &   -1.1223279448 &  -3.1148253872  &  -0.0459050276  & -16.4453742797  &  14.8248172163 \\\\\n",
       "\t13 & anovadot        &  1000.000       & 90.67278        &  -8.83058576    &   0.13349017779 &  -29.4473951794 &  -69.1024744447 & -21.9604353805  &  2.6573468      &   -1.6872468945 &  -5.8671108816  &  -0.0109795499  & -42.5112125620  &   9.8814263541 \\\\\n",
       "\t10 & anovadot        &    10.000       & 87.30887        &  -0.02852669    &   0.01108490367 &   -8.1314613538 &  -10.5797616103 &   3.7790032035  &  2.2218562      &   -0.3590986015 &   4.4898660412  &   0.0021407211  &  -7.8947856724  &  18.9730392423 \\\\\n",
       "\t32 & rbfdot          &     1.000       & 87.00306        &  -0.42067564    &   0.58153076023 &   -1.6867985593 &    3.7221663922 &  12.6751302574  & 30.1475563      &   -6.2640926105 &  17.9315722616  &  -4.6302198083  & -15.7188968881  &  26.2287199513 \\\\\n",
       "\t1 & vanilladot      &     1.000       & 86.39144        &  -0.08148382    &  -0.00110266416 &   -0.0008980539 &   -0.0016074557 &   0.0029041700  &  1.0047363      &   -0.0029852110 &  -0.0002035179  &  -0.0005504803  &  -0.0012519187  &   0.1064404601 \\\\\n",
       "\t9 & anovadot        &     1.000       & 86.39144        &  -0.37489521    &   0.00190220241 &   -1.5386990249 &   -0.9004786939 &   0.6984353224  &  2.0504384      &   -0.0270396197 &   0.7678055909  &   0.0011966941  &  -2.7190432338  &  18.6897396524 \\\\\n",
       "\t12 & vanilladot      &    10.000       & 86.39144        &  -0.08157559    &  -0.00090336713 &   -0.0007891036 &   -0.0016972133 &   0.0026113629  &  1.0050221      &   -0.0028363016 &  -0.0001569285  &  -0.0003925964  &  -0.0012784443  &   0.1064387167 \\\\\n",
       "\t18 & polydot         &     1.000       & 86.39144        &  -0.08148471    &  -0.00117790293 &   -0.0007585829 &   -0.0015830018 &   0.0030741611  &  1.0045976      &   -0.0028875480 &   0.0001266113  &  -0.0006759177  &  -0.0013468793  &   0.1064496302 \\\\\n",
       "\t19 & polydot         &    10.000       & 86.39144        &  -0.08154590    &  -0.00096471814 &   -0.0010953376 &   -0.0015706841 &   0.0026559397  &  1.0050172      &   -0.0028674084 &  -0.0002663424  &  -0.0005284851  &  -0.0013955524  &   0.1064078260 \\\\\n",
       "\t20 & polydot         &   100.000       & 86.39144        &  -0.08157716    &  -0.00109297047 &   -0.0012425741 &   -0.0015628157 &   0.0027739329  &  1.0051781      &   -0.0026901076 &  -0.0001935512  &  -0.0005270357  &  -0.0014583698  &   0.1063997443 \\\\\n",
       "\t23 & vanilladot      &   100.000       & 86.39144        &  -0.08158492    &  -0.00100653481 &   -0.0011729048 &   -0.0016261967 &   0.0030064203  &  1.0049406      &   -0.0028259432 &   0.0002600295  &  -0.0005349551  &  -0.0012283758  &   0.1063633995 \\\\\n",
       "\t24 & polydot         &     0.100       & 86.39144        &  -0.08165418    &  -0.00121245690 &   -0.0006070979 &   -0.0013956063 &   0.0033049356  &  1.0040211      &   -0.0031961704 &   0.0004545954  &  -0.0003748261  &  -0.0012915663  &   0.1064276327 \\\\\n",
       "\t25 & polydot         &     0.010       & 86.39144        &  -0.08198853    &  -0.00015007085 &   -0.0014818363 &    0.0014082877 &   0.0072863924  &  0.9916470      &   -0.0044661326 &   0.0071482901  &  -0.0005468252  &  -0.0016930686  &   0.1054824351 \\\\\n",
       "\t29 & vanilladot      &     0.100       & 86.39144        &  -0.08155226    &  -0.00116089805 &   -0.0006366002 &   -0.0015209679 &   0.0032020638  &  1.0041339      &   -0.0033773669 &   0.0002428616  &  -0.0004747021  &  -0.0011931900  &   0.1064450527 \\\\\n",
       "\t30 & vanilladot      &     0.010       & 86.39144        &  -0.08198854    &  -0.00015007376 &   -0.0014818294 &    0.0014083130 &   0.0072863886  &  0.9916470      &   -0.0044661236 &   0.0071482899  &  -0.0005468386  &  -0.0016930578  &   0.1054824270 \\\\\n",
       "\t15 & anovadot        &     0.100       & 86.23853        &  -0.03920849    &  -0.00001012938 &   -0.1554799502 &   -0.0846437302 &   0.0708553124  &  2.0403653      &   -0.0018408626 &   0.0804161637  &   0.0002612883  &  -0.2748137619  &   1.8664937362 \\\\\n",
       "\t16 & anovadot        &     0.010       & 86.23853        &  -0.14978982    &  -0.00795464422 &    0.0236412738 &    0.0247243307 &   0.2229233314  &  1.8114396      &   -0.0936787837 &   0.2969301519  &   0.0071041248  &  -0.1641909004  &   0.5162752844 \\\\\n",
       "\t21 & polydot         &  1000.000       & 86.23853        &  -0.07044332    &  -0.00022971942 &   -0.0007721707 &    0.0003367117 &   0.0004066713  &  0.9983458      &   -0.0000170658 &   0.0007563601  &   0.0002901689  &   0.0004152892  &   0.0006923412 \\\\\n",
       "\t22 & polydot         & 10000.000       & 86.23853        &  -0.07188214    &  -0.00093819800 &    0.0026823497 &    0.0082815894 &   0.0052093978  &  1.0092086      &   -0.0055107293 &   0.0086526116  &  -0.0010171646  &  -0.0030672862  &   0.0123035355 \\\\\n",
       "\t27 & vanilladot      &  1000.000       & 86.23853        &  -0.07017871    &  -0.00021491854 &    0.0007097786 &    0.0011645166 &   0.0005673024  &  0.9987192      &   -0.0005037912 &   0.0007155434  &  -0.0009130079  &   0.0007969700  &   0.0010062350 \\\\\n",
       "\t28 & vanilladot      & 10000.000       & 86.23853        &  -0.07046104    &   0.00089361671 &    0.0016125725 &   -0.0003415921 &   0.0042114213  &  1.0014528      &    0.0023573637 &   0.0052757867  &   0.0002803662  &   0.0025097815  &   0.0035920442 \\\\\n",
       "\t6 & rbfdot          &     0.100       & 85.93272        &  -0.49895859    &   0.44800542306 &    2.6930386158 &    2.9894344888 &   7.5262391844  & 18.2014277      &   -4.3052704151 &   7.2487672728  &  -0.8599054048  &  -2.7261636972  &   7.2245209456 \\\\\n",
       "\t26 & polydot         &     0.001       & 83.79205        &   0.22261555    &  -0.00215977831 &    0.0323381698 &    0.0466124485 &   0.1112231618  &  0.3753053      &   -0.2020260812 &   0.1695608466  &  -0.0049235007  &  -0.0252102660  &   0.0811897661 \\\\\n",
       "\t31 & vanilladot      &     0.001       & 83.79205        &   0.22261554    &  -0.00215977831 &    0.0323381696 &    0.0466124485 &   0.1112231617  &  0.3753053      &   -0.2020260811 &   0.1695608466  &  -0.0049235009  &  -0.0252102660  &   0.0811897661 \\\\\n",
       "\t17 & anovadot        &     0.001       & 58.86850        &   0.40814596    &  -0.00495820271 &    0.0732439801 &    0.0901967366 &   0.1794091425  &  0.4127070      &   -0.2396174520 &   0.2332018560  &  -0.0080173465  &  -0.0483230716  &   0.1017114470 \\\\\n",
       "\t7 & rbfdot          &     0.010       & 56.72783        &   0.37843714    &   0.19438004769 &    0.8585445702 &    0.9615202459 &   1.8307180198  &  4.1270703      &   -2.3961745198 &   2.3320185596  &  -0.1002168311  &  -0.5739808530  &   1.0159886099 \\\\\n",
       "\t8 & rbfdot          &     0.001       & 54.74006        &   0.94002520    &   0.01943800477 &    0.0867650055 &    0.0980760604 &   0.1828862979  &  0.4127070      &   -0.2396174520 &   0.2332018560  &  -0.0120260197  &  -0.0574456150  &   0.1015973370 \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| <!--/--> | Kernel | Cost Value | Model Accuracy | a0 | V1 | V2 | V3 | V4 | V5 | V6 | V7 | V8 | V9 | V10 | \n",
       "|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n",
       "| 5 | rbfdot          | 10000.000       | 99.23547        |  -0.63824146    | -76.93709192106 |  -73.3619155041 | -148.4894056079 | 126.5889917002  | 68.3144152      | -135.9618828193 | 119.3898440611  | -53.2487429501  | -87.7497435946  | 110.6058081105  | \n",
       "| 4 | rbfdot          |  1000.000       | 98.01223        |  -0.70466937    | -58.53155010645 |  -12.6283069575 |  -40.4821556669 | 136.9102651836  | 83.0580193      |  -95.3351838228 | 111.6392718642  | -70.8968195313  | -81.2003783239  | 100.8872134471  | \n",
       "| 3 | rbfdot          |   100.000       | 95.25994        |  -0.78805310    | -19.19664569632 |  -37.2038168337 |   -8.6463593605 |  56.0607421094  | 50.0993321      |  -24.5243903993 |  15.6733704383  | -24.1202777780  | -58.0518009098  |  51.6569482272  | \n",
       "| 2 | rbfdot          |    10.000       | 91.43731        |  -0.44146303    |  -3.01817583732 |  -17.8291518521 |    3.7185983605 |  26.3513848229  | 31.4942801      |  -12.4592419207 |  15.6013370037  |  -9.6850171040  | -32.8608467791  |  36.7662448302  | \n",
       "| 14 | anovadot        | 10000.000       | 90.82569        | -21.79686445    |   0.11124018539 | -104.1006008680 | -199.0308606894 |  71.2938356661  |  2.7999202      |   -1.9210920329 |  67.0021356644  |  -0.0514912609  | -81.3849955504  |  18.9760701171  | \n",
       "| 11 | anovadot        |   100.000       | 90.67278        |  -1.17407433    |   0.01883723961 |  -22.4979970645 |  -28.0511250529 |  -2.3583891057  |  2.5358136      |   -1.1223279448 |  -3.1148253872  |  -0.0459050276  | -16.4453742797  |  14.8248172163  | \n",
       "| 13 | anovadot        |  1000.000       | 90.67278        |  -8.83058576    |   0.13349017779 |  -29.4473951794 |  -69.1024744447 | -21.9604353805  |  2.6573468      |   -1.6872468945 |  -5.8671108816  |  -0.0109795499  | -42.5112125620  |   9.8814263541  | \n",
       "| 10 | anovadot        |    10.000       | 87.30887        |  -0.02852669    |   0.01108490367 |   -8.1314613538 |  -10.5797616103 |   3.7790032035  |  2.2218562      |   -0.3590986015 |   4.4898660412  |   0.0021407211  |  -7.8947856724  |  18.9730392423  | \n",
       "| 32 | rbfdot          |     1.000       | 87.00306        |  -0.42067564    |   0.58153076023 |   -1.6867985593 |    3.7221663922 |  12.6751302574  | 30.1475563      |   -6.2640926105 |  17.9315722616  |  -4.6302198083  | -15.7188968881  |  26.2287199513  | \n",
       "| 1 | vanilladot      |     1.000       | 86.39144        |  -0.08148382    |  -0.00110266416 |   -0.0008980539 |   -0.0016074557 |   0.0029041700  |  1.0047363      |   -0.0029852110 |  -0.0002035179  |  -0.0005504803  |  -0.0012519187  |   0.1064404601  | \n",
       "| 9 | anovadot        |     1.000       | 86.39144        |  -0.37489521    |   0.00190220241 |   -1.5386990249 |   -0.9004786939 |   0.6984353224  |  2.0504384      |   -0.0270396197 |   0.7678055909  |   0.0011966941  |  -2.7190432338  |  18.6897396524  | \n",
       "| 12 | vanilladot      |    10.000       | 86.39144        |  -0.08157559    |  -0.00090336713 |   -0.0007891036 |   -0.0016972133 |   0.0026113629  |  1.0050221      |   -0.0028363016 |  -0.0001569285  |  -0.0003925964  |  -0.0012784443  |   0.1064387167  | \n",
       "| 18 | polydot         |     1.000       | 86.39144        |  -0.08148471    |  -0.00117790293 |   -0.0007585829 |   -0.0015830018 |   0.0030741611  |  1.0045976      |   -0.0028875480 |   0.0001266113  |  -0.0006759177  |  -0.0013468793  |   0.1064496302  | \n",
       "| 19 | polydot         |    10.000       | 86.39144        |  -0.08154590    |  -0.00096471814 |   -0.0010953376 |   -0.0015706841 |   0.0026559397  |  1.0050172      |   -0.0028674084 |  -0.0002663424  |  -0.0005284851  |  -0.0013955524  |   0.1064078260  | \n",
       "| 20 | polydot         |   100.000       | 86.39144        |  -0.08157716    |  -0.00109297047 |   -0.0012425741 |   -0.0015628157 |   0.0027739329  |  1.0051781      |   -0.0026901076 |  -0.0001935512  |  -0.0005270357  |  -0.0014583698  |   0.1063997443  | \n",
       "| 23 | vanilladot      |   100.000       | 86.39144        |  -0.08158492    |  -0.00100653481 |   -0.0011729048 |   -0.0016261967 |   0.0030064203  |  1.0049406      |   -0.0028259432 |   0.0002600295  |  -0.0005349551  |  -0.0012283758  |   0.1063633995  | \n",
       "| 24 | polydot         |     0.100       | 86.39144        |  -0.08165418    |  -0.00121245690 |   -0.0006070979 |   -0.0013956063 |   0.0033049356  |  1.0040211      |   -0.0031961704 |   0.0004545954  |  -0.0003748261  |  -0.0012915663  |   0.1064276327  | \n",
       "| 25 | polydot         |     0.010       | 86.39144        |  -0.08198853    |  -0.00015007085 |   -0.0014818363 |    0.0014082877 |   0.0072863924  |  0.9916470      |   -0.0044661326 |   0.0071482901  |  -0.0005468252  |  -0.0016930686  |   0.1054824351  | \n",
       "| 29 | vanilladot      |     0.100       | 86.39144        |  -0.08155226    |  -0.00116089805 |   -0.0006366002 |   -0.0015209679 |   0.0032020638  |  1.0041339      |   -0.0033773669 |   0.0002428616  |  -0.0004747021  |  -0.0011931900  |   0.1064450527  | \n",
       "| 30 | vanilladot      |     0.010       | 86.39144        |  -0.08198854    |  -0.00015007376 |   -0.0014818294 |    0.0014083130 |   0.0072863886  |  0.9916470      |   -0.0044661236 |   0.0071482899  |  -0.0005468386  |  -0.0016930578  |   0.1054824270  | \n",
       "| 15 | anovadot        |     0.100       | 86.23853        |  -0.03920849    |  -0.00001012938 |   -0.1554799502 |   -0.0846437302 |   0.0708553124  |  2.0403653      |   -0.0018408626 |   0.0804161637  |   0.0002612883  |  -0.2748137619  |   1.8664937362  | \n",
       "| 16 | anovadot        |     0.010       | 86.23853        |  -0.14978982    |  -0.00795464422 |    0.0236412738 |    0.0247243307 |   0.2229233314  |  1.8114396      |   -0.0936787837 |   0.2969301519  |   0.0071041248  |  -0.1641909004  |   0.5162752844  | \n",
       "| 21 | polydot         |  1000.000       | 86.23853        |  -0.07044332    |  -0.00022971942 |   -0.0007721707 |    0.0003367117 |   0.0004066713  |  0.9983458      |   -0.0000170658 |   0.0007563601  |   0.0002901689  |   0.0004152892  |   0.0006923412  | \n",
       "| 22 | polydot         | 10000.000       | 86.23853        |  -0.07188214    |  -0.00093819800 |    0.0026823497 |    0.0082815894 |   0.0052093978  |  1.0092086      |   -0.0055107293 |   0.0086526116  |  -0.0010171646  |  -0.0030672862  |   0.0123035355  | \n",
       "| 27 | vanilladot      |  1000.000       | 86.23853        |  -0.07017871    |  -0.00021491854 |    0.0007097786 |    0.0011645166 |   0.0005673024  |  0.9987192      |   -0.0005037912 |   0.0007155434  |  -0.0009130079  |   0.0007969700  |   0.0010062350  | \n",
       "| 28 | vanilladot      | 10000.000       | 86.23853        |  -0.07046104    |   0.00089361671 |    0.0016125725 |   -0.0003415921 |   0.0042114213  |  1.0014528      |    0.0023573637 |   0.0052757867  |   0.0002803662  |   0.0025097815  |   0.0035920442  | \n",
       "| 6 | rbfdot          |     0.100       | 85.93272        |  -0.49895859    |   0.44800542306 |    2.6930386158 |    2.9894344888 |   7.5262391844  | 18.2014277      |   -4.3052704151 |   7.2487672728  |  -0.8599054048  |  -2.7261636972  |   7.2245209456  | \n",
       "| 26 | polydot         |     0.001       | 83.79205        |   0.22261555    |  -0.00215977831 |    0.0323381698 |    0.0466124485 |   0.1112231618  |  0.3753053      |   -0.2020260812 |   0.1695608466  |  -0.0049235007  |  -0.0252102660  |   0.0811897661  | \n",
       "| 31 | vanilladot      |     0.001       | 83.79205        |   0.22261554    |  -0.00215977831 |    0.0323381696 |    0.0466124485 |   0.1112231617  |  0.3753053      |   -0.2020260811 |   0.1695608466  |  -0.0049235009  |  -0.0252102660  |   0.0811897661  | \n",
       "| 17 | anovadot        |     0.001       | 58.86850        |   0.40814596    |  -0.00495820271 |    0.0732439801 |    0.0901967366 |   0.1794091425  |  0.4127070      |   -0.2396174520 |   0.2332018560  |  -0.0080173465  |  -0.0483230716  |   0.1017114470  | \n",
       "| 7 | rbfdot          |     0.010       | 56.72783        |   0.37843714    |   0.19438004769 |    0.8585445702 |    0.9615202459 |   1.8307180198  |  4.1270703      |   -2.3961745198 |   2.3320185596  |  -0.1002168311  |  -0.5739808530  |   1.0159886099  | \n",
       "| 8 | rbfdot          |     0.001       | 54.74006        |   0.94002520    |   0.01943800477 |    0.0867650055 |    0.0980760604 |   0.1828862979  |  0.4127070      |   -0.2396174520 |   0.2332018560  |  -0.0120260197  |  -0.0574456150  |   0.1015973370  | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "   Kernel     Cost Value Model Accuracy a0           V1             \n",
       "5  rbfdot     10000.000  99.23547        -0.63824146 -76.93709192106\n",
       "4  rbfdot      1000.000  98.01223        -0.70466937 -58.53155010645\n",
       "3  rbfdot       100.000  95.25994        -0.78805310 -19.19664569632\n",
       "2  rbfdot        10.000  91.43731        -0.44146303  -3.01817583732\n",
       "14 anovadot   10000.000  90.82569       -21.79686445   0.11124018539\n",
       "11 anovadot     100.000  90.67278        -1.17407433   0.01883723961\n",
       "13 anovadot    1000.000  90.67278        -8.83058576   0.13349017779\n",
       "10 anovadot      10.000  87.30887        -0.02852669   0.01108490367\n",
       "32 rbfdot         1.000  87.00306        -0.42067564   0.58153076023\n",
       "1  vanilladot     1.000  86.39144        -0.08148382  -0.00110266416\n",
       "9  anovadot       1.000  86.39144        -0.37489521   0.00190220241\n",
       "12 vanilladot    10.000  86.39144        -0.08157559  -0.00090336713\n",
       "18 polydot        1.000  86.39144        -0.08148471  -0.00117790293\n",
       "19 polydot       10.000  86.39144        -0.08154590  -0.00096471814\n",
       "20 polydot      100.000  86.39144        -0.08157716  -0.00109297047\n",
       "23 vanilladot   100.000  86.39144        -0.08158492  -0.00100653481\n",
       "24 polydot        0.100  86.39144        -0.08165418  -0.00121245690\n",
       "25 polydot        0.010  86.39144        -0.08198853  -0.00015007085\n",
       "29 vanilladot     0.100  86.39144        -0.08155226  -0.00116089805\n",
       "30 vanilladot     0.010  86.39144        -0.08198854  -0.00015007376\n",
       "15 anovadot       0.100  86.23853        -0.03920849  -0.00001012938\n",
       "16 anovadot       0.010  86.23853        -0.14978982  -0.00795464422\n",
       "21 polydot     1000.000  86.23853        -0.07044332  -0.00022971942\n",
       "22 polydot    10000.000  86.23853        -0.07188214  -0.00093819800\n",
       "27 vanilladot  1000.000  86.23853        -0.07017871  -0.00021491854\n",
       "28 vanilladot 10000.000  86.23853        -0.07046104   0.00089361671\n",
       "6  rbfdot         0.100  85.93272        -0.49895859   0.44800542306\n",
       "26 polydot        0.001  83.79205         0.22261555  -0.00215977831\n",
       "31 vanilladot     0.001  83.79205         0.22261554  -0.00215977831\n",
       "17 anovadot       0.001  58.86850         0.40814596  -0.00495820271\n",
       "7  rbfdot         0.010  56.72783         0.37843714   0.19438004769\n",
       "8  rbfdot         0.001  54.74006         0.94002520   0.01943800477\n",
       "   V2              V3              V4             V5         V6             \n",
       "5   -73.3619155041 -148.4894056079 126.5889917002 68.3144152 -135.9618828193\n",
       "4   -12.6283069575  -40.4821556669 136.9102651836 83.0580193  -95.3351838228\n",
       "3   -37.2038168337   -8.6463593605  56.0607421094 50.0993321  -24.5243903993\n",
       "2   -17.8291518521    3.7185983605  26.3513848229 31.4942801  -12.4592419207\n",
       "14 -104.1006008680 -199.0308606894  71.2938356661  2.7999202   -1.9210920329\n",
       "11  -22.4979970645  -28.0511250529  -2.3583891057  2.5358136   -1.1223279448\n",
       "13  -29.4473951794  -69.1024744447 -21.9604353805  2.6573468   -1.6872468945\n",
       "10   -8.1314613538  -10.5797616103   3.7790032035  2.2218562   -0.3590986015\n",
       "32   -1.6867985593    3.7221663922  12.6751302574 30.1475563   -6.2640926105\n",
       "1    -0.0008980539   -0.0016074557   0.0029041700  1.0047363   -0.0029852110\n",
       "9    -1.5386990249   -0.9004786939   0.6984353224  2.0504384   -0.0270396197\n",
       "12   -0.0007891036   -0.0016972133   0.0026113629  1.0050221   -0.0028363016\n",
       "18   -0.0007585829   -0.0015830018   0.0030741611  1.0045976   -0.0028875480\n",
       "19   -0.0010953376   -0.0015706841   0.0026559397  1.0050172   -0.0028674084\n",
       "20   -0.0012425741   -0.0015628157   0.0027739329  1.0051781   -0.0026901076\n",
       "23   -0.0011729048   -0.0016261967   0.0030064203  1.0049406   -0.0028259432\n",
       "24   -0.0006070979   -0.0013956063   0.0033049356  1.0040211   -0.0031961704\n",
       "25   -0.0014818363    0.0014082877   0.0072863924  0.9916470   -0.0044661326\n",
       "29   -0.0006366002   -0.0015209679   0.0032020638  1.0041339   -0.0033773669\n",
       "30   -0.0014818294    0.0014083130   0.0072863886  0.9916470   -0.0044661236\n",
       "15   -0.1554799502   -0.0846437302   0.0708553124  2.0403653   -0.0018408626\n",
       "16    0.0236412738    0.0247243307   0.2229233314  1.8114396   -0.0936787837\n",
       "21   -0.0007721707    0.0003367117   0.0004066713  0.9983458   -0.0000170658\n",
       "22    0.0026823497    0.0082815894   0.0052093978  1.0092086   -0.0055107293\n",
       "27    0.0007097786    0.0011645166   0.0005673024  0.9987192   -0.0005037912\n",
       "28    0.0016125725   -0.0003415921   0.0042114213  1.0014528    0.0023573637\n",
       "6     2.6930386158    2.9894344888   7.5262391844 18.2014277   -4.3052704151\n",
       "26    0.0323381698    0.0466124485   0.1112231618  0.3753053   -0.2020260812\n",
       "31    0.0323381696    0.0466124485   0.1112231617  0.3753053   -0.2020260811\n",
       "17    0.0732439801    0.0901967366   0.1794091425  0.4127070   -0.2396174520\n",
       "7     0.8585445702    0.9615202459   1.8307180198  4.1270703   -2.3961745198\n",
       "8     0.0867650055    0.0980760604   0.1828862979  0.4127070   -0.2396174520\n",
       "   V7             V8             V9             V10           \n",
       "5  119.3898440611 -53.2487429501 -87.7497435946 110.6058081105\n",
       "4  111.6392718642 -70.8968195313 -81.2003783239 100.8872134471\n",
       "3   15.6733704383 -24.1202777780 -58.0518009098  51.6569482272\n",
       "2   15.6013370037  -9.6850171040 -32.8608467791  36.7662448302\n",
       "14  67.0021356644  -0.0514912609 -81.3849955504  18.9760701171\n",
       "11  -3.1148253872  -0.0459050276 -16.4453742797  14.8248172163\n",
       "13  -5.8671108816  -0.0109795499 -42.5112125620   9.8814263541\n",
       "10   4.4898660412   0.0021407211  -7.8947856724  18.9730392423\n",
       "32  17.9315722616  -4.6302198083 -15.7188968881  26.2287199513\n",
       "1   -0.0002035179  -0.0005504803  -0.0012519187   0.1064404601\n",
       "9    0.7678055909   0.0011966941  -2.7190432338  18.6897396524\n",
       "12  -0.0001569285  -0.0003925964  -0.0012784443   0.1064387167\n",
       "18   0.0001266113  -0.0006759177  -0.0013468793   0.1064496302\n",
       "19  -0.0002663424  -0.0005284851  -0.0013955524   0.1064078260\n",
       "20  -0.0001935512  -0.0005270357  -0.0014583698   0.1063997443\n",
       "23   0.0002600295  -0.0005349551  -0.0012283758   0.1063633995\n",
       "24   0.0004545954  -0.0003748261  -0.0012915663   0.1064276327\n",
       "25   0.0071482901  -0.0005468252  -0.0016930686   0.1054824351\n",
       "29   0.0002428616  -0.0004747021  -0.0011931900   0.1064450527\n",
       "30   0.0071482899  -0.0005468386  -0.0016930578   0.1054824270\n",
       "15   0.0804161637   0.0002612883  -0.2748137619   1.8664937362\n",
       "16   0.2969301519   0.0071041248  -0.1641909004   0.5162752844\n",
       "21   0.0007563601   0.0002901689   0.0004152892   0.0006923412\n",
       "22   0.0086526116  -0.0010171646  -0.0030672862   0.0123035355\n",
       "27   0.0007155434  -0.0009130079   0.0007969700   0.0010062350\n",
       "28   0.0052757867   0.0002803662   0.0025097815   0.0035920442\n",
       "6    7.2487672728  -0.8599054048  -2.7261636972   7.2245209456\n",
       "26   0.1695608466  -0.0049235007  -0.0252102660   0.0811897661\n",
       "31   0.1695608466  -0.0049235009  -0.0252102660   0.0811897661\n",
       "17   0.2332018560  -0.0080173465  -0.0483230716   0.1017114470\n",
       "7    2.3320185596  -0.1002168311  -0.5739808530   1.0159886099\n",
       "8    0.2332018560  -0.0120260197  -0.0574456150   0.1015973370"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "options(scipen = 999)\n",
    "library(kernlab)\n",
    "library(e1071)\n",
    "library(data.table)\n",
    "library(dplyr)\n",
    "\n",
    "data <- read.table(\"credit_card_data-headers.txt\", header = TRUE)\n",
    "cost_values = c(1, 10, 100, 1000, 10000, 0.1, 0.01, 0.001)\n",
    "kernel_list = c(\"vanilladot\", \"rbfdot\", \"anovadot\", \"polydot\")\n",
    "accuracy_val <- c()\n",
    "c_val <- c()\n",
    "kernel_val <- c()\n",
    "weights <- c()\n",
    "bias <- c()\n",
    "\n",
    "for (i in 1: length(kernel_list)){\n",
    "    for (j in 1:length(cost_values)) {\n",
    "      model <- ksvm(as.matrix(data[,1:10]), as.factor(data[,11]), type = \"C-svc\", kernel =  kernel_list[i], scaled = TRUE, C=cost_values[j])\n",
    "      a<- colSums(model@xmatrix[[1]]*model@coef[[1]])\n",
    "      a0 <- model@b\n",
    "      pred <- predict(model,data[,1:10])\n",
    "      accur = sum(pred == data[,11])/nrow(data)\n",
    "      accuracy_val <- c(accuracy_val, accur)\n",
    "      c_val <- c(c_val, cost_values[j])\n",
    "      kernel_val <- c(kernel_val, kernel_list[i])\n",
    "      weights <- c(weights, a)\n",
    "      bias <- c(bias, a0)\n",
    "    }\n",
    "}\n",
    "\n",
    "df <- data.frame(kernel_val, c_val, accuracy_val*100, bias)\n",
    "names(df) <- c(\"Kernel\",\"Cost Value\",\"Model Accuracy\", \"a0\")\n",
    "\n",
    "weights1 <- as.data.frame(matrix(weights, 32, 10, byrow = T))\n",
    "df <- data.frame(kernel_val, c_val, accuracy_val*100, bias)\n",
    "names(df) <- c(\"Kernel\",\"Cost Value\",\"Model Accuracy\", \"a0\")\n",
    "df1 <- merge(df, weights1, by=0)\n",
    "drops <- c(\"Row.names\")\n",
    "df1[order(df1[\"Model Accuracy\"], decreasing = TRUE),  !(names(df1) %in% drops)]\n",
    "names(df1)[names(df1) == \"V1\"] <- \"a1\"\n",
    "names(df1)[names(df1) == \"V2\"] <- \"a2\"\n",
    "names(df1)[names(df1) == \"V3\"] <- \"a3\"\n",
    "names(df1)[names(df1) == \"V4\"] <- \"a8\"\n",
    "names(df1)[names(df1) == \"V5\"] <- \"a9\"\n",
    "names(df1)[names(df1) == \"V6\"] <- \"a10\"\n",
    "names(df1)[names(df1) == \"V7\"] <- \"a11\"\n",
    "names(df1)[names(df1) == \"V8\"] <- \"a12\"\n",
    "names(df1)[names(df1) == \"V9\"] <- \"a14\"\n",
    "names(df1)[names(df1) == \"V10\"] <- \"a15\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.2(a) Alternate method\n",
    "#### REPRODUCING THE ABOVE KSVM FUNCTION WITH 80-20 Train-Test Split for the best C-value for each of the above kernel methods. \n",
    "#### Similar to the previous cell, I have created a dataframe of kernel, cost values, accuracy, wieghts and bias vectors. I have ordered the dataframe by accuracy in descending order. \n",
    "#### The poor model accuracies could be simply a case of overfitting. In the previous cell, the model calculated predicted accuracy on data it was trained in, thus fit that data too well. But in this case, we are testing the model on a test dataset, leading to poorer model accuracy.\n",
    "#### I used the split technique used in this stackoverflow link:\n",
    "https://stackoverflow.com/questions/17200114/how-to-split-data-into-training-testing-sets-using-sample-function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th></th><th scope=col>Kernel</th><th scope=col>Cost Value</th><th scope=col>Model Accuracy</th><th scope=col>Bias</th><th scope=col>V1</th><th scope=col>V2</th><th scope=col>V3</th><th scope=col>V4</th><th scope=col>V5</th><th scope=col>V6</th><th scope=col>V7</th><th scope=col>V8</th><th scope=col>V9</th><th scope=col>V10</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>11</th><td>anovadot       </td><td>  100.000      </td><td>88.54962       </td><td> -2.66983539   </td><td>  0.15681584311</td><td> -22.4787451448</td><td> -25.3143501802</td><td>  0.0472051163 </td><td>  2.7056535    </td><td> -1.2938355861 </td><td>-2.7081839016  </td><td>  -0.0053662410</td><td>   8.0560346417</td><td>55.941934663   </td></tr>\n",
       "\t<tr><th scope=row>13</th><td>anovadot       </td><td> 1000.000      </td><td>87.78626       </td><td> -7.73772115   </td><td>  0.20149114886</td><td> -62.3291855859</td><td> -54.5352431405</td><td>-12.7373786968 </td><td>  2.9827013    </td><td> -2.0898231853 </td><td> 6.4006313533  </td><td>  -0.1176360366</td><td>  -3.0827324405</td><td>32.935693999   </td></tr>\n",
       "\t<tr><th scope=row>10</th><td>anovadot       </td><td>   10.000      </td><td>86.25954       </td><td> -0.62600087   </td><td> -0.01508102521</td><td>  -8.2171063937</td><td> -10.3621450069</td><td>  4.8201703856 </td><td>  2.3922116    </td><td> -0.4713283742 </td><td> 4.2693905074  </td><td>  -0.0364777666</td><td>   2.2456339639</td><td>28.042189422   </td></tr>\n",
       "\t<tr><th scope=row>14</th><td>anovadot       </td><td>10000.000      </td><td>86.25954       </td><td>-19.75539577   </td><td>  0.23603209871</td><td>-141.3793629830</td><td> -86.3276799089</td><td>-21.5872990256 </td><td>  2.9562049    </td><td> -1.1617634114 </td><td>24.5980393264  </td><td>  -0.0256927191</td><td>   2.4388808671</td><td>30.551785249   </td></tr>\n",
       "\t<tr><th scope=row>1</th><td>vanilladot     </td><td>    1.000      </td><td>85.49618       </td><td> -0.09739999   </td><td> -0.00107433634</td><td>  -0.0020990654</td><td>  -0.0016807507</td><td>  0.0042647148 </td><td>  1.0047152    </td><td> -0.0014298621 </td><td> 0.0006778175  </td><td>  -0.0008749079</td><td>  -0.0006219716</td><td> 0.109474517   </td></tr>\n",
       "\t<tr><th scope=row>9</th><td>anovadot       </td><td>    1.000      </td><td>85.49618       </td><td> -0.58628080   </td><td> -0.00106433194</td><td>  -1.3151866751</td><td>  -1.2742658817</td><td>  1.1078581244 </td><td>  2.0630098    </td><td> -0.0314735481 </td><td> 1.1954989194  </td><td>  -0.0014836391</td><td>   0.1971462562</td><td>18.194215276   </td></tr>\n",
       "\t<tr><th scope=row>12</th><td>vanilladot     </td><td>   10.000      </td><td>85.49618       </td><td> -0.09726800   </td><td> -0.00117581013</td><td>  -0.0021618342</td><td>  -0.0020185757</td><td>  0.0046171297 </td><td>  1.0049805    </td><td> -0.0014183085 </td><td> 0.0008231443  </td><td>  -0.0007462537</td><td>  -0.0008029273</td><td> 0.109607261   </td></tr>\n",
       "\t<tr><th scope=row>15</th><td>anovadot       </td><td>    0.100      </td><td>85.49618       </td><td> -0.06077585   </td><td> -0.00003832392</td><td>  -0.1290328776</td><td>  -0.1146214540</td><td>  0.1085304610 </td><td>  2.0423360    </td><td> -0.0020072354 </td><td> 0.1228293593  </td><td>   0.0001899328</td><td>   0.0057615266</td><td> 1.819438455   </td></tr>\n",
       "\t<tr><th scope=row>18</th><td>polydot        </td><td>    1.000      </td><td>85.49618       </td><td> -0.09729321   </td><td> -0.00117728585</td><td>  -0.0020144465</td><td>  -0.0018992961</td><td>  0.0045339600 </td><td>  1.0050025    </td><td> -0.0014153320 </td><td> 0.0007296392  </td><td>  -0.0009024631</td><td>  -0.0007261950</td><td> 0.109582268   </td></tr>\n",
       "\t<tr><th scope=row>19</th><td>polydot        </td><td>   10.000      </td><td>85.49618       </td><td> -0.09740113   </td><td> -0.00106567026</td><td>  -0.0020013082</td><td>  -0.0016111113</td><td>  0.0040460833 </td><td>  1.0047940    </td><td> -0.0013414215 </td><td> 0.0008051959  </td><td>  -0.0006193236</td><td>  -0.0005425273</td><td> 0.109493994   </td></tr>\n",
       "\t<tr><th scope=row>20</th><td>polydot        </td><td>  100.000      </td><td>85.49618       </td><td> -0.09710195   </td><td> -0.00131673502</td><td>  -0.0020654587</td><td>  -0.0017846763</td><td>  0.0043327819 </td><td>  1.0048605    </td><td> -0.0015718702 </td><td> 0.0007936751  </td><td>  -0.0008130210</td><td>  -0.0006381422</td><td> 0.109549643   </td></tr>\n",
       "\t<tr><th scope=row>21</th><td>polydot        </td><td> 1000.000      </td><td>85.49618       </td><td> -0.08588901   </td><td>  0.00016348864</td><td>   0.0005152847</td><td>  -0.0002312576</td><td>  0.0004359906 </td><td>  0.9981452    </td><td> -0.0007591235 </td><td>-0.0003784779  </td><td>  -0.0001090195</td><td>  -0.0013848391</td><td> 0.002007222   </td></tr>\n",
       "\t<tr><th scope=row>22</th><td>polydot        </td><td>10000.000      </td><td>85.49618       </td><td> -0.08526746   </td><td> -0.00195078227</td><td>   0.0038529897</td><td>   0.0014114186</td><td>  0.0064801281 </td><td>  0.9974943    </td><td> -0.0066724315 </td><td> 0.0028467984  </td><td>   0.0007967961</td><td>   0.0002173934</td><td> 0.015966710   </td></tr>\n",
       "\t<tr><th scope=row>23</th><td>vanilladot     </td><td>  100.000      </td><td>85.49618       </td><td> -0.09721200   </td><td> -0.00107671853</td><td>  -0.0020602805</td><td>  -0.0017033717</td><td>  0.0039784939 </td><td>  1.0050680    </td><td> -0.0013306276 </td><td> 0.0010139426  </td><td>  -0.0008834927</td><td>  -0.0008899141</td><td> 0.109546051   </td></tr>\n",
       "\t<tr><th scope=row>24</th><td>polydot        </td><td>    0.100      </td><td>85.49618       </td><td> -0.09834222   </td><td> -0.00136258606</td><td>  -0.0014045740</td><td>  -0.0013012923</td><td>  0.0035179549 </td><td>  1.0029793    </td><td> -0.0015227011 </td><td> 0.0012167951  </td><td>  -0.0010151078</td><td>  -0.0005148377</td><td> 0.109284793   </td></tr>\n",
       "\t<tr><th scope=row>25</th><td>polydot        </td><td>    0.010      </td><td>85.49618       </td><td> -0.09754501   </td><td> -0.00106415440</td><td>  -0.0018193278</td><td>   0.0039899832</td><td>  0.0112262163 </td><td>  0.9845057    </td><td> -0.0045651121 </td><td> 0.0132411261  </td><td>  -0.0013412661</td><td>  -0.0017490769</td><td> 0.108133757   </td></tr>\n",
       "\t<tr><th scope=row>27</th><td>vanilladot     </td><td> 1000.000      </td><td>85.49618       </td><td> -0.08606945   </td><td>  0.00019008636</td><td>   0.0002998667</td><td>   0.0002165888</td><td>  0.0005074091 </td><td>  0.9974718    </td><td> -0.0010701424 </td><td> 0.0001358001  </td><td>   0.0003256009</td><td>  -0.0002015549</td><td> 0.002293208   </td></tr>\n",
       "\t<tr><th scope=row>28</th><td>vanilladot     </td><td>10000.000      </td><td>85.49618       </td><td> -0.08871878   </td><td> -0.00175092685</td><td>   0.0036540001</td><td>  -0.0045134983</td><td>  0.0047415736 </td><td>  1.0014557    </td><td> -0.0046510630 </td><td>-0.0039905075  </td><td>   0.0093537353</td><td>  -0.0038367386</td><td> 0.020070512   </td></tr>\n",
       "\t<tr><th scope=row>29</th><td>vanilladot     </td><td>    0.100      </td><td>85.49618       </td><td> -0.09838218   </td><td> -0.00129612346</td><td>  -0.0013626013</td><td>  -0.0013522761</td><td>  0.0036024550 </td><td>  1.0029996    </td><td> -0.0013764231 </td><td> 0.0013908346  </td><td>  -0.0012586429</td><td>  -0.0003743683</td><td> 0.109298420   </td></tr>\n",
       "\t<tr><th scope=row>30</th><td>vanilladot     </td><td>    0.010      </td><td>85.49618       </td><td> -0.09754502   </td><td> -0.00106413518</td><td>  -0.0018193307</td><td>   0.0039899708</td><td>  0.0112261930 </td><td>  0.9845058    </td><td> -0.0045650894 </td><td> 0.0132411241  </td><td>  -0.0013412554</td><td>  -0.0017490725</td><td> 0.108133761   </td></tr>\n",
       "\t<tr><th scope=row>32</th><td>rbfdot         </td><td>    1.000      </td><td>85.49618       </td><td> -0.40957213   </td><td> -2.04442860421</td><td>   0.3866227893</td><td>   2.0225476631</td><td> 15.9647791351 </td><td> 28.5402266    </td><td> -4.3547055903 </td><td>11.8275955472  </td><td>  -4.7819414120</td><td> -12.9450410990</td><td>22.651801756   </td></tr>\n",
       "\t<tr><th scope=row>6</th><td>rbfdot         </td><td>    0.100      </td><td>84.73282       </td><td> -0.47822488   </td><td>  0.31827953404</td><td>   2.4905123762</td><td>   2.5146231116</td><td>  7.0057362013 </td><td> 16.4593585    </td><td> -3.6803645681 </td><td> 6.0006790835  </td><td>  -0.8942029717</td><td>  -2.1380078563</td><td> 5.934030797   </td></tr>\n",
       "\t<tr><th scope=row>26</th><td>polydot        </td><td>    0.001      </td><td>83.96947       </td><td>  0.29164928   </td><td> -0.00530297781</td><td>   0.0338661041</td><td>   0.0605962903</td><td>  0.1069222905 </td><td>  0.3127378    </td><td> -0.1692829115 </td><td> 0.1690951436  </td><td>  -0.0144024909</td><td>  -0.0243384946</td><td> 0.074742593   </td></tr>\n",
       "\t<tr><th scope=row>31</th><td>vanilladot     </td><td>    0.001      </td><td>83.96947       </td><td>  0.29164928   </td><td> -0.00530297871</td><td>   0.0338661035</td><td>   0.0605962901</td><td>  0.1069222902 </td><td>  0.3127378    </td><td> -0.1692829113 </td><td> 0.1690951434  </td><td>  -0.0144024924</td><td>  -0.0243384951</td><td> 0.074742593   </td></tr>\n",
       "\t<tr><th scope=row>2</th><td>rbfdot         </td><td>   10.000      </td><td>81.67939       </td><td> -0.44259458   </td><td>-12.39184196070</td><td> -15.6758957448</td><td>   4.5748748970</td><td> 32.3394387855 </td><td> 28.2963105    </td><td>-11.1424352373 </td><td> 6.9161416617  </td><td> -14.7896413061</td><td> -21.8751490405</td><td>25.555458074   </td></tr>\n",
       "\t<tr><th scope=row>3</th><td>rbfdot         </td><td>  100.000      </td><td>80.15267       </td><td> -0.65145613   </td><td>-10.20000313384</td><td> -27.7218582700</td><td>  -5.6766002324</td><td> 61.9698756554 </td><td> 56.4209665    </td><td>-35.9550980178 </td><td>19.4269015279  </td><td> -33.1804391188</td><td> -40.4000732789</td><td>32.637980732   </td></tr>\n",
       "\t<tr><th scope=row>4</th><td>rbfdot         </td><td> 1000.000      </td><td>79.38931       </td><td> -1.23879027   </td><td>-23.58541446144</td><td> -21.2492144343</td><td>-124.1571731667</td><td> 93.6442944458 </td><td>120.3203259    </td><td>-98.8614411584 </td><td>67.5218013625  </td><td> -70.5557411642</td><td> -91.8757139614</td><td>71.571613036   </td></tr>\n",
       "\t<tr><th scope=row>5</th><td>rbfdot         </td><td>10000.000      </td><td>77.86260       </td><td> -2.08644510   </td><td>-14.95093912344</td><td> -79.1986580085</td><td>-236.4777338798</td><td>110.9654196790 </td><td>123.6127404    </td><td>-92.2066614085 </td><td>66.5787293509  </td><td>-103.1205470008</td><td>-207.2527496789</td><td>59.299568925   </td></tr>\n",
       "\t<tr><th scope=row>16</th><td>anovadot       </td><td>    0.010      </td><td>74.04580       </td><td> -0.19864180   </td><td> -0.01535700189</td><td>   0.0350049147</td><td>   0.0463721843</td><td>  0.2712795280 </td><td>  1.6279941    </td><td> -0.1850471213 </td><td> 0.3349900166  </td><td>  -0.0282622631</td><td>  -0.0982700978</td><td> 0.519044756   </td></tr>\n",
       "\t<tr><th scope=row>17</th><td>anovadot       </td><td>    0.001      </td><td>66.41221       </td><td>  0.50465689   </td><td> -0.00444237055</td><td>   0.0598649558</td><td>   0.0879332478</td><td>  0.1533199079 </td><td>  0.3329171    </td><td> -0.1895751923 </td><td> 0.2051009490  </td><td>  -0.0160248484</td><td>  -0.0413726458</td><td> 0.081815879   </td></tr>\n",
       "\t<tr><th scope=row>7</th><td>rbfdot         </td><td>    0.010      </td><td>54.96183       </td><td>  0.47706905   </td><td>  0.12963618386</td><td>   0.7170428903</td><td>   0.9166762262</td><td>  1.5681187657 </td><td>  3.3291707    </td><td> -1.8957519235 </td><td> 2.0510094901  </td><td>  -0.1999159406</td><td>  -0.4674860145</td><td> 0.818320130   </td></tr>\n",
       "\t<tr><th scope=row>8</th><td>rbfdot         </td><td>    0.001      </td><td>54.96183       </td><td>  0.94854979   </td><td>  0.01512422145</td><td>   0.0715753134</td><td>   0.0920495128</td><td>  0.1571744811 </td><td>  0.3329171    </td><td> -0.1895751923 </td><td> 0.2051009490  </td><td>  -0.0200310605</td><td>  -0.0479139639</td><td> 0.081832013   </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|llllllllllllll}\n",
       "  & Kernel & Cost Value & Model Accuracy & Bias & V1 & V2 & V3 & V4 & V5 & V6 & V7 & V8 & V9 & V10\\\\\n",
       "\\hline\n",
       "\t11 & anovadot        &   100.000       & 88.54962        &  -2.66983539    &   0.15681584311 &  -22.4787451448 &  -25.3143501802 &   0.0472051163  &   2.7056535     &  -1.2938355861  & -2.7081839016   &   -0.0053662410 &    8.0560346417 & 55.941934663   \\\\\n",
       "\t13 & anovadot        &  1000.000       & 87.78626        &  -7.73772115    &   0.20149114886 &  -62.3291855859 &  -54.5352431405 & -12.7373786968  &   2.9827013     &  -2.0898231853  &  6.4006313533   &   -0.1176360366 &   -3.0827324405 & 32.935693999   \\\\\n",
       "\t10 & anovadot        &    10.000       & 86.25954        &  -0.62600087    &  -0.01508102521 &   -8.2171063937 &  -10.3621450069 &   4.8201703856  &   2.3922116     &  -0.4713283742  &  4.2693905074   &   -0.0364777666 &    2.2456339639 & 28.042189422   \\\\\n",
       "\t14 & anovadot        & 10000.000       & 86.25954        & -19.75539577    &   0.23603209871 & -141.3793629830 &  -86.3276799089 & -21.5872990256  &   2.9562049     &  -1.1617634114  & 24.5980393264   &   -0.0256927191 &    2.4388808671 & 30.551785249   \\\\\n",
       "\t1 & vanilladot      &     1.000       & 85.49618        &  -0.09739999    &  -0.00107433634 &   -0.0020990654 &   -0.0016807507 &   0.0042647148  &   1.0047152     &  -0.0014298621  &  0.0006778175   &   -0.0008749079 &   -0.0006219716 &  0.109474517   \\\\\n",
       "\t9 & anovadot        &     1.000       & 85.49618        &  -0.58628080    &  -0.00106433194 &   -1.3151866751 &   -1.2742658817 &   1.1078581244  &   2.0630098     &  -0.0314735481  &  1.1954989194   &   -0.0014836391 &    0.1971462562 & 18.194215276   \\\\\n",
       "\t12 & vanilladot      &    10.000       & 85.49618        &  -0.09726800    &  -0.00117581013 &   -0.0021618342 &   -0.0020185757 &   0.0046171297  &   1.0049805     &  -0.0014183085  &  0.0008231443   &   -0.0007462537 &   -0.0008029273 &  0.109607261   \\\\\n",
       "\t15 & anovadot        &     0.100       & 85.49618        &  -0.06077585    &  -0.00003832392 &   -0.1290328776 &   -0.1146214540 &   0.1085304610  &   2.0423360     &  -0.0020072354  &  0.1228293593   &    0.0001899328 &    0.0057615266 &  1.819438455   \\\\\n",
       "\t18 & polydot         &     1.000       & 85.49618        &  -0.09729321    &  -0.00117728585 &   -0.0020144465 &   -0.0018992961 &   0.0045339600  &   1.0050025     &  -0.0014153320  &  0.0007296392   &   -0.0009024631 &   -0.0007261950 &  0.109582268   \\\\\n",
       "\t19 & polydot         &    10.000       & 85.49618        &  -0.09740113    &  -0.00106567026 &   -0.0020013082 &   -0.0016111113 &   0.0040460833  &   1.0047940     &  -0.0013414215  &  0.0008051959   &   -0.0006193236 &   -0.0005425273 &  0.109493994   \\\\\n",
       "\t20 & polydot         &   100.000       & 85.49618        &  -0.09710195    &  -0.00131673502 &   -0.0020654587 &   -0.0017846763 &   0.0043327819  &   1.0048605     &  -0.0015718702  &  0.0007936751   &   -0.0008130210 &   -0.0006381422 &  0.109549643   \\\\\n",
       "\t21 & polydot         &  1000.000       & 85.49618        &  -0.08588901    &   0.00016348864 &    0.0005152847 &   -0.0002312576 &   0.0004359906  &   0.9981452     &  -0.0007591235  & -0.0003784779   &   -0.0001090195 &   -0.0013848391 &  0.002007222   \\\\\n",
       "\t22 & polydot         & 10000.000       & 85.49618        &  -0.08526746    &  -0.00195078227 &    0.0038529897 &    0.0014114186 &   0.0064801281  &   0.9974943     &  -0.0066724315  &  0.0028467984   &    0.0007967961 &    0.0002173934 &  0.015966710   \\\\\n",
       "\t23 & vanilladot      &   100.000       & 85.49618        &  -0.09721200    &  -0.00107671853 &   -0.0020602805 &   -0.0017033717 &   0.0039784939  &   1.0050680     &  -0.0013306276  &  0.0010139426   &   -0.0008834927 &   -0.0008899141 &  0.109546051   \\\\\n",
       "\t24 & polydot         &     0.100       & 85.49618        &  -0.09834222    &  -0.00136258606 &   -0.0014045740 &   -0.0013012923 &   0.0035179549  &   1.0029793     &  -0.0015227011  &  0.0012167951   &   -0.0010151078 &   -0.0005148377 &  0.109284793   \\\\\n",
       "\t25 & polydot         &     0.010       & 85.49618        &  -0.09754501    &  -0.00106415440 &   -0.0018193278 &    0.0039899832 &   0.0112262163  &   0.9845057     &  -0.0045651121  &  0.0132411261   &   -0.0013412661 &   -0.0017490769 &  0.108133757   \\\\\n",
       "\t27 & vanilladot      &  1000.000       & 85.49618        &  -0.08606945    &   0.00019008636 &    0.0002998667 &    0.0002165888 &   0.0005074091  &   0.9974718     &  -0.0010701424  &  0.0001358001   &    0.0003256009 &   -0.0002015549 &  0.002293208   \\\\\n",
       "\t28 & vanilladot      & 10000.000       & 85.49618        &  -0.08871878    &  -0.00175092685 &    0.0036540001 &   -0.0045134983 &   0.0047415736  &   1.0014557     &  -0.0046510630  & -0.0039905075   &    0.0093537353 &   -0.0038367386 &  0.020070512   \\\\\n",
       "\t29 & vanilladot      &     0.100       & 85.49618        &  -0.09838218    &  -0.00129612346 &   -0.0013626013 &   -0.0013522761 &   0.0036024550  &   1.0029996     &  -0.0013764231  &  0.0013908346   &   -0.0012586429 &   -0.0003743683 &  0.109298420   \\\\\n",
       "\t30 & vanilladot      &     0.010       & 85.49618        &  -0.09754502    &  -0.00106413518 &   -0.0018193307 &    0.0039899708 &   0.0112261930  &   0.9845058     &  -0.0045650894  &  0.0132411241   &   -0.0013412554 &   -0.0017490725 &  0.108133761   \\\\\n",
       "\t32 & rbfdot          &     1.000       & 85.49618        &  -0.40957213    &  -2.04442860421 &    0.3866227893 &    2.0225476631 &  15.9647791351  &  28.5402266     &  -4.3547055903  & 11.8275955472   &   -4.7819414120 &  -12.9450410990 & 22.651801756   \\\\\n",
       "\t6 & rbfdot          &     0.100       & 84.73282        &  -0.47822488    &   0.31827953404 &    2.4905123762 &    2.5146231116 &   7.0057362013  &  16.4593585     &  -3.6803645681  &  6.0006790835   &   -0.8942029717 &   -2.1380078563 &  5.934030797   \\\\\n",
       "\t26 & polydot         &     0.001       & 83.96947        &   0.29164928    &  -0.00530297781 &    0.0338661041 &    0.0605962903 &   0.1069222905  &   0.3127378     &  -0.1692829115  &  0.1690951436   &   -0.0144024909 &   -0.0243384946 &  0.074742593   \\\\\n",
       "\t31 & vanilladot      &     0.001       & 83.96947        &   0.29164928    &  -0.00530297871 &    0.0338661035 &    0.0605962901 &   0.1069222902  &   0.3127378     &  -0.1692829113  &  0.1690951434   &   -0.0144024924 &   -0.0243384951 &  0.074742593   \\\\\n",
       "\t2 & rbfdot          &    10.000       & 81.67939        &  -0.44259458    & -12.39184196070 &  -15.6758957448 &    4.5748748970 &  32.3394387855  &  28.2963105     & -11.1424352373  &  6.9161416617   &  -14.7896413061 &  -21.8751490405 & 25.555458074   \\\\\n",
       "\t3 & rbfdot          &   100.000       & 80.15267        &  -0.65145613    & -10.20000313384 &  -27.7218582700 &   -5.6766002324 &  61.9698756554  &  56.4209665     & -35.9550980178  & 19.4269015279   &  -33.1804391188 &  -40.4000732789 & 32.637980732   \\\\\n",
       "\t4 & rbfdot          &  1000.000       & 79.38931        &  -1.23879027    & -23.58541446144 &  -21.2492144343 & -124.1571731667 &  93.6442944458  & 120.3203259     & -98.8614411584  & 67.5218013625   &  -70.5557411642 &  -91.8757139614 & 71.571613036   \\\\\n",
       "\t5 & rbfdot          & 10000.000       & 77.86260        &  -2.08644510    & -14.95093912344 &  -79.1986580085 & -236.4777338798 & 110.9654196790  & 123.6127404     & -92.2066614085  & 66.5787293509   & -103.1205470008 & -207.2527496789 & 59.299568925   \\\\\n",
       "\t16 & anovadot        &     0.010       & 74.04580        &  -0.19864180    &  -0.01535700189 &    0.0350049147 &    0.0463721843 &   0.2712795280  &   1.6279941     &  -0.1850471213  &  0.3349900166   &   -0.0282622631 &   -0.0982700978 &  0.519044756   \\\\\n",
       "\t17 & anovadot        &     0.001       & 66.41221        &   0.50465689    &  -0.00444237055 &    0.0598649558 &    0.0879332478 &   0.1533199079  &   0.3329171     &  -0.1895751923  &  0.2051009490   &   -0.0160248484 &   -0.0413726458 &  0.081815879   \\\\\n",
       "\t7 & rbfdot          &     0.010       & 54.96183        &   0.47706905    &   0.12963618386 &    0.7170428903 &    0.9166762262 &   1.5681187657  &   3.3291707     &  -1.8957519235  &  2.0510094901   &   -0.1999159406 &   -0.4674860145 &  0.818320130   \\\\\n",
       "\t8 & rbfdot          &     0.001       & 54.96183        &   0.94854979    &   0.01512422145 &    0.0715753134 &    0.0920495128 &   0.1571744811  &   0.3329171     &  -0.1895751923  &  0.2051009490   &   -0.0200310605 &   -0.0479139639 &  0.081832013   \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| <!--/--> | Kernel | Cost Value | Model Accuracy | Bias | V1 | V2 | V3 | V4 | V5 | V6 | V7 | V8 | V9 | V10 | \n",
       "|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n",
       "| 11 | anovadot        |   100.000       | 88.54962        |  -2.66983539    |   0.15681584311 |  -22.4787451448 |  -25.3143501802 |   0.0472051163  |   2.7056535     |  -1.2938355861  | -2.7081839016   |   -0.0053662410 |    8.0560346417 | 55.941934663    | \n",
       "| 13 | anovadot        |  1000.000       | 87.78626        |  -7.73772115    |   0.20149114886 |  -62.3291855859 |  -54.5352431405 | -12.7373786968  |   2.9827013     |  -2.0898231853  |  6.4006313533   |   -0.1176360366 |   -3.0827324405 | 32.935693999    | \n",
       "| 10 | anovadot        |    10.000       | 86.25954        |  -0.62600087    |  -0.01508102521 |   -8.2171063937 |  -10.3621450069 |   4.8201703856  |   2.3922116     |  -0.4713283742  |  4.2693905074   |   -0.0364777666 |    2.2456339639 | 28.042189422    | \n",
       "| 14 | anovadot        | 10000.000       | 86.25954        | -19.75539577    |   0.23603209871 | -141.3793629830 |  -86.3276799089 | -21.5872990256  |   2.9562049     |  -1.1617634114  | 24.5980393264   |   -0.0256927191 |    2.4388808671 | 30.551785249    | \n",
       "| 1 | vanilladot      |     1.000       | 85.49618        |  -0.09739999    |  -0.00107433634 |   -0.0020990654 |   -0.0016807507 |   0.0042647148  |   1.0047152     |  -0.0014298621  |  0.0006778175   |   -0.0008749079 |   -0.0006219716 |  0.109474517    | \n",
       "| 9 | anovadot        |     1.000       | 85.49618        |  -0.58628080    |  -0.00106433194 |   -1.3151866751 |   -1.2742658817 |   1.1078581244  |   2.0630098     |  -0.0314735481  |  1.1954989194   |   -0.0014836391 |    0.1971462562 | 18.194215276    | \n",
       "| 12 | vanilladot      |    10.000       | 85.49618        |  -0.09726800    |  -0.00117581013 |   -0.0021618342 |   -0.0020185757 |   0.0046171297  |   1.0049805     |  -0.0014183085  |  0.0008231443   |   -0.0007462537 |   -0.0008029273 |  0.109607261    | \n",
       "| 15 | anovadot        |     0.100       | 85.49618        |  -0.06077585    |  -0.00003832392 |   -0.1290328776 |   -0.1146214540 |   0.1085304610  |   2.0423360     |  -0.0020072354  |  0.1228293593   |    0.0001899328 |    0.0057615266 |  1.819438455    | \n",
       "| 18 | polydot         |     1.000       | 85.49618        |  -0.09729321    |  -0.00117728585 |   -0.0020144465 |   -0.0018992961 |   0.0045339600  |   1.0050025     |  -0.0014153320  |  0.0007296392   |   -0.0009024631 |   -0.0007261950 |  0.109582268    | \n",
       "| 19 | polydot         |    10.000       | 85.49618        |  -0.09740113    |  -0.00106567026 |   -0.0020013082 |   -0.0016111113 |   0.0040460833  |   1.0047940     |  -0.0013414215  |  0.0008051959   |   -0.0006193236 |   -0.0005425273 |  0.109493994    | \n",
       "| 20 | polydot         |   100.000       | 85.49618        |  -0.09710195    |  -0.00131673502 |   -0.0020654587 |   -0.0017846763 |   0.0043327819  |   1.0048605     |  -0.0015718702  |  0.0007936751   |   -0.0008130210 |   -0.0006381422 |  0.109549643    | \n",
       "| 21 | polydot         |  1000.000       | 85.49618        |  -0.08588901    |   0.00016348864 |    0.0005152847 |   -0.0002312576 |   0.0004359906  |   0.9981452     |  -0.0007591235  | -0.0003784779   |   -0.0001090195 |   -0.0013848391 |  0.002007222    | \n",
       "| 22 | polydot         | 10000.000       | 85.49618        |  -0.08526746    |  -0.00195078227 |    0.0038529897 |    0.0014114186 |   0.0064801281  |   0.9974943     |  -0.0066724315  |  0.0028467984   |    0.0007967961 |    0.0002173934 |  0.015966710    | \n",
       "| 23 | vanilladot      |   100.000       | 85.49618        |  -0.09721200    |  -0.00107671853 |   -0.0020602805 |   -0.0017033717 |   0.0039784939  |   1.0050680     |  -0.0013306276  |  0.0010139426   |   -0.0008834927 |   -0.0008899141 |  0.109546051    | \n",
       "| 24 | polydot         |     0.100       | 85.49618        |  -0.09834222    |  -0.00136258606 |   -0.0014045740 |   -0.0013012923 |   0.0035179549  |   1.0029793     |  -0.0015227011  |  0.0012167951   |   -0.0010151078 |   -0.0005148377 |  0.109284793    | \n",
       "| 25 | polydot         |     0.010       | 85.49618        |  -0.09754501    |  -0.00106415440 |   -0.0018193278 |    0.0039899832 |   0.0112262163  |   0.9845057     |  -0.0045651121  |  0.0132411261   |   -0.0013412661 |   -0.0017490769 |  0.108133757    | \n",
       "| 27 | vanilladot      |  1000.000       | 85.49618        |  -0.08606945    |   0.00019008636 |    0.0002998667 |    0.0002165888 |   0.0005074091  |   0.9974718     |  -0.0010701424  |  0.0001358001   |    0.0003256009 |   -0.0002015549 |  0.002293208    | \n",
       "| 28 | vanilladot      | 10000.000       | 85.49618        |  -0.08871878    |  -0.00175092685 |    0.0036540001 |   -0.0045134983 |   0.0047415736  |   1.0014557     |  -0.0046510630  | -0.0039905075   |    0.0093537353 |   -0.0038367386 |  0.020070512    | \n",
       "| 29 | vanilladot      |     0.100       | 85.49618        |  -0.09838218    |  -0.00129612346 |   -0.0013626013 |   -0.0013522761 |   0.0036024550  |   1.0029996     |  -0.0013764231  |  0.0013908346   |   -0.0012586429 |   -0.0003743683 |  0.109298420    | \n",
       "| 30 | vanilladot      |     0.010       | 85.49618        |  -0.09754502    |  -0.00106413518 |   -0.0018193307 |    0.0039899708 |   0.0112261930  |   0.9845058     |  -0.0045650894  |  0.0132411241   |   -0.0013412554 |   -0.0017490725 |  0.108133761    | \n",
       "| 32 | rbfdot          |     1.000       | 85.49618        |  -0.40957213    |  -2.04442860421 |    0.3866227893 |    2.0225476631 |  15.9647791351  |  28.5402266     |  -4.3547055903  | 11.8275955472   |   -4.7819414120 |  -12.9450410990 | 22.651801756    | \n",
       "| 6 | rbfdot          |     0.100       | 84.73282        |  -0.47822488    |   0.31827953404 |    2.4905123762 |    2.5146231116 |   7.0057362013  |  16.4593585     |  -3.6803645681  |  6.0006790835   |   -0.8942029717 |   -2.1380078563 |  5.934030797    | \n",
       "| 26 | polydot         |     0.001       | 83.96947        |   0.29164928    |  -0.00530297781 |    0.0338661041 |    0.0605962903 |   0.1069222905  |   0.3127378     |  -0.1692829115  |  0.1690951436   |   -0.0144024909 |   -0.0243384946 |  0.074742593    | \n",
       "| 31 | vanilladot      |     0.001       | 83.96947        |   0.29164928    |  -0.00530297871 |    0.0338661035 |    0.0605962901 |   0.1069222902  |   0.3127378     |  -0.1692829113  |  0.1690951434   |   -0.0144024924 |   -0.0243384951 |  0.074742593    | \n",
       "| 2 | rbfdot          |    10.000       | 81.67939        |  -0.44259458    | -12.39184196070 |  -15.6758957448 |    4.5748748970 |  32.3394387855  |  28.2963105     | -11.1424352373  |  6.9161416617   |  -14.7896413061 |  -21.8751490405 | 25.555458074    | \n",
       "| 3 | rbfdot          |   100.000       | 80.15267        |  -0.65145613    | -10.20000313384 |  -27.7218582700 |   -5.6766002324 |  61.9698756554  |  56.4209665     | -35.9550980178  | 19.4269015279   |  -33.1804391188 |  -40.4000732789 | 32.637980732    | \n",
       "| 4 | rbfdot          |  1000.000       | 79.38931        |  -1.23879027    | -23.58541446144 |  -21.2492144343 | -124.1571731667 |  93.6442944458  | 120.3203259     | -98.8614411584  | 67.5218013625   |  -70.5557411642 |  -91.8757139614 | 71.571613036    | \n",
       "| 5 | rbfdot          | 10000.000       | 77.86260        |  -2.08644510    | -14.95093912344 |  -79.1986580085 | -236.4777338798 | 110.9654196790  | 123.6127404     | -92.2066614085  | 66.5787293509   | -103.1205470008 | -207.2527496789 | 59.299568925    | \n",
       "| 16 | anovadot        |     0.010       | 74.04580        |  -0.19864180    |  -0.01535700189 |    0.0350049147 |    0.0463721843 |   0.2712795280  |   1.6279941     |  -0.1850471213  |  0.3349900166   |   -0.0282622631 |   -0.0982700978 |  0.519044756    | \n",
       "| 17 | anovadot        |     0.001       | 66.41221        |   0.50465689    |  -0.00444237055 |    0.0598649558 |    0.0879332478 |   0.1533199079  |   0.3329171     |  -0.1895751923  |  0.2051009490   |   -0.0160248484 |   -0.0413726458 |  0.081815879    | \n",
       "| 7 | rbfdot          |     0.010       | 54.96183        |   0.47706905    |   0.12963618386 |    0.7170428903 |    0.9166762262 |   1.5681187657  |   3.3291707     |  -1.8957519235  |  2.0510094901   |   -0.1999159406 |   -0.4674860145 |  0.818320130    | \n",
       "| 8 | rbfdot          |     0.001       | 54.96183        |   0.94854979    |   0.01512422145 |    0.0715753134 |    0.0920495128 |   0.1571744811  |   0.3329171     |  -0.1895751923  |  0.2051009490   |   -0.0200310605 |   -0.0479139639 |  0.081832013    | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "   Kernel     Cost Value Model Accuracy Bias         V1             \n",
       "11 anovadot     100.000  88.54962        -2.66983539   0.15681584311\n",
       "13 anovadot    1000.000  87.78626        -7.73772115   0.20149114886\n",
       "10 anovadot      10.000  86.25954        -0.62600087  -0.01508102521\n",
       "14 anovadot   10000.000  86.25954       -19.75539577   0.23603209871\n",
       "1  vanilladot     1.000  85.49618        -0.09739999  -0.00107433634\n",
       "9  anovadot       1.000  85.49618        -0.58628080  -0.00106433194\n",
       "12 vanilladot    10.000  85.49618        -0.09726800  -0.00117581013\n",
       "15 anovadot       0.100  85.49618        -0.06077585  -0.00003832392\n",
       "18 polydot        1.000  85.49618        -0.09729321  -0.00117728585\n",
       "19 polydot       10.000  85.49618        -0.09740113  -0.00106567026\n",
       "20 polydot      100.000  85.49618        -0.09710195  -0.00131673502\n",
       "21 polydot     1000.000  85.49618        -0.08588901   0.00016348864\n",
       "22 polydot    10000.000  85.49618        -0.08526746  -0.00195078227\n",
       "23 vanilladot   100.000  85.49618        -0.09721200  -0.00107671853\n",
       "24 polydot        0.100  85.49618        -0.09834222  -0.00136258606\n",
       "25 polydot        0.010  85.49618        -0.09754501  -0.00106415440\n",
       "27 vanilladot  1000.000  85.49618        -0.08606945   0.00019008636\n",
       "28 vanilladot 10000.000  85.49618        -0.08871878  -0.00175092685\n",
       "29 vanilladot     0.100  85.49618        -0.09838218  -0.00129612346\n",
       "30 vanilladot     0.010  85.49618        -0.09754502  -0.00106413518\n",
       "32 rbfdot         1.000  85.49618        -0.40957213  -2.04442860421\n",
       "6  rbfdot         0.100  84.73282        -0.47822488   0.31827953404\n",
       "26 polydot        0.001  83.96947         0.29164928  -0.00530297781\n",
       "31 vanilladot     0.001  83.96947         0.29164928  -0.00530297871\n",
       "2  rbfdot        10.000  81.67939        -0.44259458 -12.39184196070\n",
       "3  rbfdot       100.000  80.15267        -0.65145613 -10.20000313384\n",
       "4  rbfdot      1000.000  79.38931        -1.23879027 -23.58541446144\n",
       "5  rbfdot     10000.000  77.86260        -2.08644510 -14.95093912344\n",
       "16 anovadot       0.010  74.04580        -0.19864180  -0.01535700189\n",
       "17 anovadot       0.001  66.41221         0.50465689  -0.00444237055\n",
       "7  rbfdot         0.010  54.96183         0.47706905   0.12963618386\n",
       "8  rbfdot         0.001  54.96183         0.94854979   0.01512422145\n",
       "   V2              V3              V4             V5          V6            \n",
       "11  -22.4787451448  -25.3143501802   0.0472051163   2.7056535  -1.2938355861\n",
       "13  -62.3291855859  -54.5352431405 -12.7373786968   2.9827013  -2.0898231853\n",
       "10   -8.2171063937  -10.3621450069   4.8201703856   2.3922116  -0.4713283742\n",
       "14 -141.3793629830  -86.3276799089 -21.5872990256   2.9562049  -1.1617634114\n",
       "1    -0.0020990654   -0.0016807507   0.0042647148   1.0047152  -0.0014298621\n",
       "9    -1.3151866751   -1.2742658817   1.1078581244   2.0630098  -0.0314735481\n",
       "12   -0.0021618342   -0.0020185757   0.0046171297   1.0049805  -0.0014183085\n",
       "15   -0.1290328776   -0.1146214540   0.1085304610   2.0423360  -0.0020072354\n",
       "18   -0.0020144465   -0.0018992961   0.0045339600   1.0050025  -0.0014153320\n",
       "19   -0.0020013082   -0.0016111113   0.0040460833   1.0047940  -0.0013414215\n",
       "20   -0.0020654587   -0.0017846763   0.0043327819   1.0048605  -0.0015718702\n",
       "21    0.0005152847   -0.0002312576   0.0004359906   0.9981452  -0.0007591235\n",
       "22    0.0038529897    0.0014114186   0.0064801281   0.9974943  -0.0066724315\n",
       "23   -0.0020602805   -0.0017033717   0.0039784939   1.0050680  -0.0013306276\n",
       "24   -0.0014045740   -0.0013012923   0.0035179549   1.0029793  -0.0015227011\n",
       "25   -0.0018193278    0.0039899832   0.0112262163   0.9845057  -0.0045651121\n",
       "27    0.0002998667    0.0002165888   0.0005074091   0.9974718  -0.0010701424\n",
       "28    0.0036540001   -0.0045134983   0.0047415736   1.0014557  -0.0046510630\n",
       "29   -0.0013626013   -0.0013522761   0.0036024550   1.0029996  -0.0013764231\n",
       "30   -0.0018193307    0.0039899708   0.0112261930   0.9845058  -0.0045650894\n",
       "32    0.3866227893    2.0225476631  15.9647791351  28.5402266  -4.3547055903\n",
       "6     2.4905123762    2.5146231116   7.0057362013  16.4593585  -3.6803645681\n",
       "26    0.0338661041    0.0605962903   0.1069222905   0.3127378  -0.1692829115\n",
       "31    0.0338661035    0.0605962901   0.1069222902   0.3127378  -0.1692829113\n",
       "2   -15.6758957448    4.5748748970  32.3394387855  28.2963105 -11.1424352373\n",
       "3   -27.7218582700   -5.6766002324  61.9698756554  56.4209665 -35.9550980178\n",
       "4   -21.2492144343 -124.1571731667  93.6442944458 120.3203259 -98.8614411584\n",
       "5   -79.1986580085 -236.4777338798 110.9654196790 123.6127404 -92.2066614085\n",
       "16    0.0350049147    0.0463721843   0.2712795280   1.6279941  -0.1850471213\n",
       "17    0.0598649558    0.0879332478   0.1533199079   0.3329171  -0.1895751923\n",
       "7     0.7170428903    0.9166762262   1.5681187657   3.3291707  -1.8957519235\n",
       "8     0.0715753134    0.0920495128   0.1571744811   0.3329171  -0.1895751923\n",
       "   V7            V8              V9              V10         \n",
       "11 -2.7081839016   -0.0053662410    8.0560346417 55.941934663\n",
       "13  6.4006313533   -0.1176360366   -3.0827324405 32.935693999\n",
       "10  4.2693905074   -0.0364777666    2.2456339639 28.042189422\n",
       "14 24.5980393264   -0.0256927191    2.4388808671 30.551785249\n",
       "1   0.0006778175   -0.0008749079   -0.0006219716  0.109474517\n",
       "9   1.1954989194   -0.0014836391    0.1971462562 18.194215276\n",
       "12  0.0008231443   -0.0007462537   -0.0008029273  0.109607261\n",
       "15  0.1228293593    0.0001899328    0.0057615266  1.819438455\n",
       "18  0.0007296392   -0.0009024631   -0.0007261950  0.109582268\n",
       "19  0.0008051959   -0.0006193236   -0.0005425273  0.109493994\n",
       "20  0.0007936751   -0.0008130210   -0.0006381422  0.109549643\n",
       "21 -0.0003784779   -0.0001090195   -0.0013848391  0.002007222\n",
       "22  0.0028467984    0.0007967961    0.0002173934  0.015966710\n",
       "23  0.0010139426   -0.0008834927   -0.0008899141  0.109546051\n",
       "24  0.0012167951   -0.0010151078   -0.0005148377  0.109284793\n",
       "25  0.0132411261   -0.0013412661   -0.0017490769  0.108133757\n",
       "27  0.0001358001    0.0003256009   -0.0002015549  0.002293208\n",
       "28 -0.0039905075    0.0093537353   -0.0038367386  0.020070512\n",
       "29  0.0013908346   -0.0012586429   -0.0003743683  0.109298420\n",
       "30  0.0132411241   -0.0013412554   -0.0017490725  0.108133761\n",
       "32 11.8275955472   -4.7819414120  -12.9450410990 22.651801756\n",
       "6   6.0006790835   -0.8942029717   -2.1380078563  5.934030797\n",
       "26  0.1690951436   -0.0144024909   -0.0243384946  0.074742593\n",
       "31  0.1690951434   -0.0144024924   -0.0243384951  0.074742593\n",
       "2   6.9161416617  -14.7896413061  -21.8751490405 25.555458074\n",
       "3  19.4269015279  -33.1804391188  -40.4000732789 32.637980732\n",
       "4  67.5218013625  -70.5557411642  -91.8757139614 71.571613036\n",
       "5  66.5787293509 -103.1205470008 -207.2527496789 59.299568925\n",
       "16  0.3349900166   -0.0282622631   -0.0982700978  0.519044756\n",
       "17  0.2051009490   -0.0160248484   -0.0413726458  0.081815879\n",
       "7   2.0510094901   -0.1999159406   -0.4674860145  0.818320130\n",
       "8   0.2051009490   -0.0200310605   -0.0479139639  0.081832013"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "options(scipen = 999)\n",
    "library(kernlab)\n",
    "library(e1071)\n",
    "library(data.table)\n",
    "library(caTools)\n",
    "require(caTools)\n",
    "set.seed(101)\n",
    "\n",
    "sample = sample.split(data$R1, SplitRatio = 0.80)\n",
    "x_data_train <- subset(data[,1:10], sample == TRUE)\n",
    "y_data_train <- subset(data[,11], sample == TRUE)\n",
    "x_data_test <- subset(data[,1:10], sample == FALSE)\n",
    "y_data_test <- subset(data[,11], sample == FALSE)\n",
    "\n",
    "data <- read.table(\"credit_card_data-headers.txt\", header = TRUE)\n",
    "cost_values = c(1, 10, 100, 1000, 10000, 0.1, 0.01, 0.001)\n",
    "kernel_list = c(\"vanilladot\", \"rbfdot\", \"anovadot\", \"polydot\")\n",
    "accuracy_val <- c()\n",
    "c_val <- c()\n",
    "kernel_val <- c()\n",
    "weights <- c()\n",
    "bias <- c()\n",
    "\n",
    "for (i in 1: length(kernel_list)){\n",
    "    for (j in 1:length(cost_values)) {\n",
    "      model <- ksvm(as.matrix(x_data_train), as.factor(y_data_train), type = \"C-svc\", kernel =  kernel_list[i], scaled = TRUE, C=cost_values[j])\n",
    "      a<- colSums(model@xmatrix[[1]]*model@coef[[1]])\n",
    "      a0 <- model@b\n",
    "      pred <- predict(model,x_data_test)\n",
    "      accur = sum(pred == y_data_test)/length(y_data_test)\n",
    "\n",
    "      accuracy_val <- c(accuracy_val, accur)\n",
    "      c_val <- c(c_val, cost_values[j])\n",
    "      kernel_val <- c(kernel_val, kernel_list[i])\n",
    "      weights <- c(weights, a)\n",
    "      bias <- c(bias, a0)\n",
    "    }\n",
    "}\n",
    "weights1 <- as.data.frame(matrix(weights, 32, 10, byrow = T))\n",
    "df <- data.frame(kernel_val, c_val, accuracy_val*100, bias)\n",
    "names(df) <- c(\"Kernel\",\"Cost Value\",\"Model Accuracy\", \"Bias\")\n",
    "df1 <- merge(df, weights1, by=0)\n",
    "drops <- c(\"Row.names\")\n",
    "df1[order(df1[\"Model Accuracy\"], decreasing = TRUE),  !(names(df1) %in% drops)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.3:\n",
    "##### Using the k-nearest-neighbors classification function kknn contained in the R kknn package, suggest a good value of k, and show how well it classifies that data points in the full data set. Dont forget to scale the data (scale=TRUE in kknn). Note that kknn will read the responses as continuous, and return the fraction of the k closest responses that are 1 (rather than the most common response, 1 or 0).\n",
    "\n",
    "##### Stepwise explanation to the problem approach: \n",
    "```\n",
    "In this problem, I have created vectors of different kernels and k values. The logic is similar to train.kknn (leave-one-out cross validation). The method looks at all the data looks at all the data except for the i-th row of data. To do this, I loop through the rows of data, over different values of k. \n",
    "\n",
    "Since the predicted values have integers with decimal places, I round them to get predicted values of 0 or 1. For each kernel and each k value, I am appending the model accuracy, k value and name of the kernel to its own empty vectors. After this step, I am joining these vectors to a dataframe (sorted by accuracy - descending).\n",
    "\n",
    "```\n",
    "#### Using this approach, 12 and 15 seem to be good values of k, and the model accuracy for both the values of k is 85.32110 %. I get the best possible accuracy when using \"optimal\" kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th></th><th scope=col>Kernel</th><th scope=col>K Value</th><th scope=col>Model Accuracy</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>262</th><td>optimal    </td><td>12         </td><td>85.32110   </td></tr>\n",
       "\t<tr><th scope=row>265</th><td>optimal    </td><td>15         </td><td>85.32110   </td></tr>\n",
       "\t<tr><th scope=row>60</th><td>triangular </td><td>10         </td><td>85.16820   </td></tr>\n",
       "\t<tr><th scope=row>255</th><td>optimal    </td><td> 5         </td><td>85.16820   </td></tr>\n",
       "\t<tr><th scope=row>261</th><td>optimal    </td><td>11         </td><td>85.16820   </td></tr>\n",
       "\t<tr><th scope=row>263</th><td>optimal    </td><td>13         </td><td>85.16820   </td></tr>\n",
       "\t<tr><th scope=row>264</th><td>optimal    </td><td>14         </td><td>85.16820   </td></tr>\n",
       "\t<tr><th scope=row>266</th><td>optimal    </td><td>16         </td><td>85.16820   </td></tr>\n",
       "\t<tr><th scope=row>267</th><td>optimal    </td><td>17         </td><td>85.16820   </td></tr>\n",
       "\t<tr><th scope=row>268</th><td>optimal    </td><td>18         </td><td>85.16820   </td></tr>\n",
       "\t<tr><th scope=row>22</th><td>rectangular</td><td>22         </td><td>85.01529   </td></tr>\n",
       "\t<tr><th scope=row>44</th><td>rectangular</td><td>44         </td><td>85.01529   </td></tr>\n",
       "\t<tr><th scope=row>61</th><td>triangular </td><td>11         </td><td>85.01529   </td></tr>\n",
       "\t<tr><th scope=row>62</th><td>triangular </td><td>12         </td><td>85.01529   </td></tr>\n",
       "\t<tr><th scope=row>64</th><td>triangular </td><td>14         </td><td>85.01529   </td></tr>\n",
       "\t<tr><th scope=row>142</th><td>inv        </td><td>42         </td><td>85.01529   </td></tr>\n",
       "\t<tr><th scope=row>158</th><td>gaussian   </td><td> 8         </td><td>85.01529   </td></tr>\n",
       "\t<tr><th scope=row>222</th><td>rank       </td><td>22         </td><td>85.01529   </td></tr>\n",
       "\t<tr><th scope=row>244</th><td>rank       </td><td>44         </td><td>85.01529   </td></tr>\n",
       "\t<tr><th scope=row>260</th><td>optimal    </td><td>10         </td><td>85.01529   </td></tr>\n",
       "\t<tr><th scope=row>269</th><td>optimal    </td><td>19         </td><td>85.01529   </td></tr>\n",
       "\t<tr><th scope=row>270</th><td>optimal    </td><td>20         </td><td>85.01529   </td></tr>\n",
       "\t<tr><th scope=row>48</th><td>rectangular</td><td>48         </td><td>84.86239   </td></tr>\n",
       "\t<tr><th scope=row>63</th><td>triangular </td><td>13         </td><td>84.86239   </td></tr>\n",
       "\t<tr><th scope=row>65</th><td>triangular </td><td>15         </td><td>84.86239   </td></tr>\n",
       "\t<tr><th scope=row>66</th><td>triangular </td><td>16         </td><td>84.86239   </td></tr>\n",
       "\t<tr><th scope=row>67</th><td>triangular </td><td>17         </td><td>84.86239   </td></tr>\n",
       "\t<tr><th scope=row>106</th><td>inv        </td><td> 6         </td><td>84.86239   </td></tr>\n",
       "\t<tr><th scope=row>143</th><td>inv        </td><td>43         </td><td>84.86239   </td></tr>\n",
       "\t<tr><th scope=row>248</th><td>rank       </td><td>48         </td><td>84.86239   </td></tr>\n",
       "\t<tr><th scope=row>...</th><td>...</td><td>...</td><td>...</td></tr>\n",
       "\t<tr><th scope=row>178</th><td>gaussian   </td><td>28         </td><td>82.72171   </td></tr>\n",
       "\t<tr><th scope=row>4</th><td>rectangular</td><td> 4         </td><td>82.56881   </td></tr>\n",
       "\t<tr><th scope=row>15</th><td>rectangular</td><td>15         </td><td>82.56881   </td></tr>\n",
       "\t<tr><th scope=row>176</th><td>gaussian   </td><td>26         </td><td>82.56881   </td></tr>\n",
       "\t<tr><th scope=row>204</th><td>rank       </td><td> 4         </td><td>82.56881   </td></tr>\n",
       "\t<tr><th scope=row>215</th><td>rank       </td><td>15         </td><td>82.56881   </td></tr>\n",
       "\t<tr><th scope=row>3</th><td>rectangular</td><td> 3         </td><td>82.26300   </td></tr>\n",
       "\t<tr><th scope=row>17</th><td>rectangular</td><td>17         </td><td>82.26300   </td></tr>\n",
       "\t<tr><th scope=row>19</th><td>rectangular</td><td>19         </td><td>82.26300   </td></tr>\n",
       "\t<tr><th scope=row>103</th><td>inv        </td><td> 3         </td><td>82.26300   </td></tr>\n",
       "\t<tr><th scope=row>153</th><td>gaussian   </td><td> 3         </td><td>82.26300   </td></tr>\n",
       "\t<tr><th scope=row>203</th><td>rank       </td><td> 3         </td><td>82.26300   </td></tr>\n",
       "\t<tr><th scope=row>217</th><td>rank       </td><td>17         </td><td>82.26300   </td></tr>\n",
       "\t<tr><th scope=row>219</th><td>rank       </td><td>19         </td><td>82.26300   </td></tr>\n",
       "\t<tr><th scope=row>54</th><td>triangular </td><td> 4         </td><td>81.95719   </td></tr>\n",
       "\t<tr><th scope=row>1</th><td>rectangular</td><td> 1         </td><td>81.49847   </td></tr>\n",
       "\t<tr><th scope=row>51</th><td>triangular </td><td> 1         </td><td>81.49847   </td></tr>\n",
       "\t<tr><th scope=row>52</th><td>triangular </td><td> 2         </td><td>81.49847   </td></tr>\n",
       "\t<tr><th scope=row>101</th><td>inv        </td><td> 1         </td><td>81.49847   </td></tr>\n",
       "\t<tr><th scope=row>102</th><td>inv        </td><td> 2         </td><td>81.49847   </td></tr>\n",
       "\t<tr><th scope=row>151</th><td>gaussian   </td><td> 1         </td><td>81.49847   </td></tr>\n",
       "\t<tr><th scope=row>152</th><td>gaussian   </td><td> 2         </td><td>81.49847   </td></tr>\n",
       "\t<tr><th scope=row>201</th><td>rank       </td><td> 1         </td><td>81.49847   </td></tr>\n",
       "\t<tr><th scope=row>251</th><td>optimal    </td><td> 1         </td><td>81.49847   </td></tr>\n",
       "\t<tr><th scope=row>252</th><td>optimal    </td><td> 2         </td><td>81.49847   </td></tr>\n",
       "\t<tr><th scope=row>253</th><td>optimal    </td><td> 3         </td><td>81.49847   </td></tr>\n",
       "\t<tr><th scope=row>254</th><td>optimal    </td><td> 4         </td><td>81.49847   </td></tr>\n",
       "\t<tr><th scope=row>53</th><td>triangular </td><td> 3         </td><td>80.88685   </td></tr>\n",
       "\t<tr><th scope=row>2</th><td>rectangular</td><td> 2         </td><td>78.59327   </td></tr>\n",
       "\t<tr><th scope=row>202</th><td>rank       </td><td> 2         </td><td>78.59327   </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|lll}\n",
       "  & Kernel & K Value & Model Accuracy\\\\\n",
       "\\hline\n",
       "\t262 & optimal     & 12          & 85.32110   \\\\\n",
       "\t265 & optimal     & 15          & 85.32110   \\\\\n",
       "\t60 & triangular  & 10          & 85.16820   \\\\\n",
       "\t255 & optimal     &  5          & 85.16820   \\\\\n",
       "\t261 & optimal     & 11          & 85.16820   \\\\\n",
       "\t263 & optimal     & 13          & 85.16820   \\\\\n",
       "\t264 & optimal     & 14          & 85.16820   \\\\\n",
       "\t266 & optimal     & 16          & 85.16820   \\\\\n",
       "\t267 & optimal     & 17          & 85.16820   \\\\\n",
       "\t268 & optimal     & 18          & 85.16820   \\\\\n",
       "\t22 & rectangular & 22          & 85.01529   \\\\\n",
       "\t44 & rectangular & 44          & 85.01529   \\\\\n",
       "\t61 & triangular  & 11          & 85.01529   \\\\\n",
       "\t62 & triangular  & 12          & 85.01529   \\\\\n",
       "\t64 & triangular  & 14          & 85.01529   \\\\\n",
       "\t142 & inv         & 42          & 85.01529   \\\\\n",
       "\t158 & gaussian    &  8          & 85.01529   \\\\\n",
       "\t222 & rank        & 22          & 85.01529   \\\\\n",
       "\t244 & rank        & 44          & 85.01529   \\\\\n",
       "\t260 & optimal     & 10          & 85.01529   \\\\\n",
       "\t269 & optimal     & 19          & 85.01529   \\\\\n",
       "\t270 & optimal     & 20          & 85.01529   \\\\\n",
       "\t48 & rectangular & 48          & 84.86239   \\\\\n",
       "\t63 & triangular  & 13          & 84.86239   \\\\\n",
       "\t65 & triangular  & 15          & 84.86239   \\\\\n",
       "\t66 & triangular  & 16          & 84.86239   \\\\\n",
       "\t67 & triangular  & 17          & 84.86239   \\\\\n",
       "\t106 & inv         &  6          & 84.86239   \\\\\n",
       "\t143 & inv         & 43          & 84.86239   \\\\\n",
       "\t248 & rank        & 48          & 84.86239   \\\\\n",
       "\t... & ... & ... & ...\\\\\n",
       "\t178 & gaussian    & 28          & 82.72171   \\\\\n",
       "\t4 & rectangular &  4          & 82.56881   \\\\\n",
       "\t15 & rectangular & 15          & 82.56881   \\\\\n",
       "\t176 & gaussian    & 26          & 82.56881   \\\\\n",
       "\t204 & rank        &  4          & 82.56881   \\\\\n",
       "\t215 & rank        & 15          & 82.56881   \\\\\n",
       "\t3 & rectangular &  3          & 82.26300   \\\\\n",
       "\t17 & rectangular & 17          & 82.26300   \\\\\n",
       "\t19 & rectangular & 19          & 82.26300   \\\\\n",
       "\t103 & inv         &  3          & 82.26300   \\\\\n",
       "\t153 & gaussian    &  3          & 82.26300   \\\\\n",
       "\t203 & rank        &  3          & 82.26300   \\\\\n",
       "\t217 & rank        & 17          & 82.26300   \\\\\n",
       "\t219 & rank        & 19          & 82.26300   \\\\\n",
       "\t54 & triangular  &  4          & 81.95719   \\\\\n",
       "\t1 & rectangular &  1          & 81.49847   \\\\\n",
       "\t51 & triangular  &  1          & 81.49847   \\\\\n",
       "\t52 & triangular  &  2          & 81.49847   \\\\\n",
       "\t101 & inv         &  1          & 81.49847   \\\\\n",
       "\t102 & inv         &  2          & 81.49847   \\\\\n",
       "\t151 & gaussian    &  1          & 81.49847   \\\\\n",
       "\t152 & gaussian    &  2          & 81.49847   \\\\\n",
       "\t201 & rank        &  1          & 81.49847   \\\\\n",
       "\t251 & optimal     &  1          & 81.49847   \\\\\n",
       "\t252 & optimal     &  2          & 81.49847   \\\\\n",
       "\t253 & optimal     &  3          & 81.49847   \\\\\n",
       "\t254 & optimal     &  4          & 81.49847   \\\\\n",
       "\t53 & triangular  &  3          & 80.88685   \\\\\n",
       "\t2 & rectangular &  2          & 78.59327   \\\\\n",
       "\t202 & rank        &  2          & 78.59327   \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| <!--/--> | Kernel | K Value | Model Accuracy | \n",
       "|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n",
       "| 262 | optimal     | 12          | 85.32110    | \n",
       "| 265 | optimal     | 15          | 85.32110    | \n",
       "| 60 | triangular  | 10          | 85.16820    | \n",
       "| 255 | optimal     |  5          | 85.16820    | \n",
       "| 261 | optimal     | 11          | 85.16820    | \n",
       "| 263 | optimal     | 13          | 85.16820    | \n",
       "| 264 | optimal     | 14          | 85.16820    | \n",
       "| 266 | optimal     | 16          | 85.16820    | \n",
       "| 267 | optimal     | 17          | 85.16820    | \n",
       "| 268 | optimal     | 18          | 85.16820    | \n",
       "| 22 | rectangular | 22          | 85.01529    | \n",
       "| 44 | rectangular | 44          | 85.01529    | \n",
       "| 61 | triangular  | 11          | 85.01529    | \n",
       "| 62 | triangular  | 12          | 85.01529    | \n",
       "| 64 | triangular  | 14          | 85.01529    | \n",
       "| 142 | inv         | 42          | 85.01529    | \n",
       "| 158 | gaussian    |  8          | 85.01529    | \n",
       "| 222 | rank        | 22          | 85.01529    | \n",
       "| 244 | rank        | 44          | 85.01529    | \n",
       "| 260 | optimal     | 10          | 85.01529    | \n",
       "| 269 | optimal     | 19          | 85.01529    | \n",
       "| 270 | optimal     | 20          | 85.01529    | \n",
       "| 48 | rectangular | 48          | 84.86239    | \n",
       "| 63 | triangular  | 13          | 84.86239    | \n",
       "| 65 | triangular  | 15          | 84.86239    | \n",
       "| 66 | triangular  | 16          | 84.86239    | \n",
       "| 67 | triangular  | 17          | 84.86239    | \n",
       "| 106 | inv         |  6          | 84.86239    | \n",
       "| 143 | inv         | 43          | 84.86239    | \n",
       "| 248 | rank        | 48          | 84.86239    | \n",
       "| ... | ... | ... | ... | \n",
       "| 178 | gaussian    | 28          | 82.72171    | \n",
       "| 4 | rectangular |  4          | 82.56881    | \n",
       "| 15 | rectangular | 15          | 82.56881    | \n",
       "| 176 | gaussian    | 26          | 82.56881    | \n",
       "| 204 | rank        |  4          | 82.56881    | \n",
       "| 215 | rank        | 15          | 82.56881    | \n",
       "| 3 | rectangular |  3          | 82.26300    | \n",
       "| 17 | rectangular | 17          | 82.26300    | \n",
       "| 19 | rectangular | 19          | 82.26300    | \n",
       "| 103 | inv         |  3          | 82.26300    | \n",
       "| 153 | gaussian    |  3          | 82.26300    | \n",
       "| 203 | rank        |  3          | 82.26300    | \n",
       "| 217 | rank        | 17          | 82.26300    | \n",
       "| 219 | rank        | 19          | 82.26300    | \n",
       "| 54 | triangular  |  4          | 81.95719    | \n",
       "| 1 | rectangular |  1          | 81.49847    | \n",
       "| 51 | triangular  |  1          | 81.49847    | \n",
       "| 52 | triangular  |  2          | 81.49847    | \n",
       "| 101 | inv         |  1          | 81.49847    | \n",
       "| 102 | inv         |  2          | 81.49847    | \n",
       "| 151 | gaussian    |  1          | 81.49847    | \n",
       "| 152 | gaussian    |  2          | 81.49847    | \n",
       "| 201 | rank        |  1          | 81.49847    | \n",
       "| 251 | optimal     |  1          | 81.49847    | \n",
       "| 252 | optimal     |  2          | 81.49847    | \n",
       "| 253 | optimal     |  3          | 81.49847    | \n",
       "| 254 | optimal     |  4          | 81.49847    | \n",
       "| 53 | triangular  |  3          | 80.88685    | \n",
       "| 2 | rectangular |  2          | 78.59327    | \n",
       "| 202 | rank        |  2          | 78.59327    | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "    Kernel      K Value Model Accuracy\n",
       "262 optimal     12      85.32110      \n",
       "265 optimal     15      85.32110      \n",
       "60  triangular  10      85.16820      \n",
       "255 optimal      5      85.16820      \n",
       "261 optimal     11      85.16820      \n",
       "263 optimal     13      85.16820      \n",
       "264 optimal     14      85.16820      \n",
       "266 optimal     16      85.16820      \n",
       "267 optimal     17      85.16820      \n",
       "268 optimal     18      85.16820      \n",
       "22  rectangular 22      85.01529      \n",
       "44  rectangular 44      85.01529      \n",
       "61  triangular  11      85.01529      \n",
       "62  triangular  12      85.01529      \n",
       "64  triangular  14      85.01529      \n",
       "142 inv         42      85.01529      \n",
       "158 gaussian     8      85.01529      \n",
       "222 rank        22      85.01529      \n",
       "244 rank        44      85.01529      \n",
       "260 optimal     10      85.01529      \n",
       "269 optimal     19      85.01529      \n",
       "270 optimal     20      85.01529      \n",
       "48  rectangular 48      84.86239      \n",
       "63  triangular  13      84.86239      \n",
       "65  triangular  15      84.86239      \n",
       "66  triangular  16      84.86239      \n",
       "67  triangular  17      84.86239      \n",
       "106 inv          6      84.86239      \n",
       "143 inv         43      84.86239      \n",
       "248 rank        48      84.86239      \n",
       "... ...         ...     ...           \n",
       "178 gaussian    28      82.72171      \n",
       "4   rectangular  4      82.56881      \n",
       "15  rectangular 15      82.56881      \n",
       "176 gaussian    26      82.56881      \n",
       "204 rank         4      82.56881      \n",
       "215 rank        15      82.56881      \n",
       "3   rectangular  3      82.26300      \n",
       "17  rectangular 17      82.26300      \n",
       "19  rectangular 19      82.26300      \n",
       "103 inv          3      82.26300      \n",
       "153 gaussian     3      82.26300      \n",
       "203 rank         3      82.26300      \n",
       "217 rank        17      82.26300      \n",
       "219 rank        19      82.26300      \n",
       "54  triangular   4      81.95719      \n",
       "1   rectangular  1      81.49847      \n",
       "51  triangular   1      81.49847      \n",
       "52  triangular   2      81.49847      \n",
       "101 inv          1      81.49847      \n",
       "102 inv          2      81.49847      \n",
       "151 gaussian     1      81.49847      \n",
       "152 gaussian     2      81.49847      \n",
       "201 rank         1      81.49847      \n",
       "251 optimal      1      81.49847      \n",
       "252 optimal      2      81.49847      \n",
       "253 optimal      3      81.49847      \n",
       "254 optimal      4      81.49847      \n",
       "53  triangular   3      80.88685      \n",
       "2   rectangular  2      78.59327      \n",
       "202 rank         2      78.59327      "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "library(kernlab)\n",
    "library(kknn)\n",
    "\n",
    "data <- read.table(\"credit_card_data-headers.txt\", header = TRUE)\n",
    "\n",
    "k_list <- c(1:50)\n",
    "kernel_list <- c(\"rectangular\" , \"triangular\", \"inv\", \"gaussian\", \"rank\", \"optimal\")\n",
    "\n",
    "accuracy_val <- c()\n",
    "k_val <- c()\n",
    "kernel_val <- c()\n",
    "pred <- rep(0, nrow(data))\n",
    "\n",
    "for (m in 1:length(kernel_list)){\n",
    "    for (i in 1:length(k_list)) {\n",
    "        for (j in 1:nrow(data)) {\n",
    "            kknn_model <- kknn(R1~., data[-j,1:11], data[j,1:11], k = k_list[i], distance = 2, scale = TRUE, kernel = kernel_list[m])\n",
    "            pred[j] <- round(fitted(kknn_model))\n",
    "        }\n",
    "        accur = sum(pred == data[,11])/length(data[,11])\n",
    "        accuracy_val <- c(accuracy_val, accur)\n",
    "        k_val <- c(k_val, k_list[i])\n",
    "        kernel_val <- c(kernel_val, kernel_list[m])\n",
    "    }    \n",
    "}\n",
    "\n",
    "df <- data.frame(kernel_val, k_val, accuracy_val*100)\n",
    "names(df) <- c(\"Kernel\",\"K Value\",\"Model Accuracy\")\n",
    "df[order(df[\"Model Accuracy\"], decreasing = TRUE), ]\n",
    "\n",
    "# cat(\"Max Accuracy is:\", max(accuracy[1:length(k_list)]), \",\\nand the corresponding value of k is: \", k_list[which.max(accuracy[1:length(k_list)])])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3.1 (a):\n",
    "#### Problem:\n",
    "```\n",
    "Using the same data set (credit_card_data.txt or credit_card_data-headers.txt) as in Question 2.2, use the ksvm or kknn function to find a good classifier:\n",
    "Using cross-validation (do this for the k-nearest-neighbors model; SVM is optional)\n",
    "```\n",
    "#### Solution:\n",
    "````\n",
    "Here I am using train.kknn and cv.kknn methods. \n",
    "train.kknn - Training of kknn method via leave-one-out crossvalidation\n",
    "cv.kknn - k-fold cross-validation, where k is the number number of data points (HERE I AM USING 10). Cross validates 1 row with k-1 rows. \n",
    "\n",
    "I am also plotting all the \"train.kknn\" models in one plot. This shows mean squared error values vs. k-values. From this plot visualization, it is not exactly possible to ascertain what the lowest MSE is, or what kernel has the lowest MSE. However, it can be seen that the lowest MSE seems to occur around k=22.\n",
    "\n",
    "From the train.kknn approach, attributes(model) shows that the Minimal mean squared error is 10.61155 % for k = 22 and for the \"inv\" Kernel.\n",
    "\n",
    "````  \n",
    "\n",
    "\n",
    "##### Websites I referred to other than documentation, to work on this problem:\n",
    "```\n",
    "1. train.kknn - http://course1.winona.edu/bdeppa/Stat%20425/Handouts/Section%2013%20-%20Nearest%20Neighbor%20Classification.docx\n",
    "2. cv.kknn - https://www.kaggle.com/jaybee79/sf-crime-class-knn-s-only-benchmark\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "\"package 'data.table' was built under R version 3.5.3\""
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------BEGIN train.kknn method -------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "Call:\n",
       "train.kknn(formula = R1 ~ ., data = data, kmax = 200, distance = 2,     kernel = kernels, scale = TRUE)\n",
       "\n",
       "Type of response variable: continuous\n",
       "minimal mean absolute error: 0.1850153\n",
       "Minimal mean squared error: 0.1061155\n",
       "Best kernel: inv\n",
       "Best k: 22"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best Kernel is: inv , Best K Value is: 22 , and the accuracy is: 100 %\n",
      "\n",
      "-------------------------------------END train.kknn method -------------------------------------------\n",
      "\n",
      "\n",
      "-------------------------------------BEGIN cv.kknn method -------------------------------------------\n",
      "\n",
      "For rectangular kernel, model accuracy is: 85.01529 % and corresponding k-value is: 38\n",
      "For triangular kernel, model accuracy is: 85.3211 % and corresponding k-value is: 38\n",
      "For epanechnikov kernel, model accuracy is: 85.47401 % and corresponding k-value is: 38\n",
      "For biweight kernel, model accuracy is: 85.47401 % and corresponding k-value is: 38\n",
      "For triweight kernel, model accuracy is: 85.77982 % and corresponding k-value is: 38\n",
      "For cos kernel, model accuracy is: 85.93272 % and corresponding k-value is: 38\n",
      "For inv kernel, model accuracy is: 85.93272 % and corresponding k-value is: 38\n",
      "For gaussian kernel, model accuracy is: 85.93272 % and corresponding k-value is: 38\n",
      "For optimal kernel, model accuracy is: 85.93272 % and corresponding k-value is: 38\n",
      "-------------------------------------END cv.kknn method -------------------------------------------\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAARVBMVEUAAAAAAP8AzQAA//9N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\nIBIACYBIACQAIgGQgP8D3BsSESAA494AAAAASUVORK5CYII=",
      "text/plain": [
       "Plot with title \"Kernels using train.kknn\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "library(kernlab)\n",
    "library(kknn)\n",
    "library(data.table)\n",
    "library(e1071)\n",
    "library(caTools)\n",
    "require(caTools)\n",
    "set.seed(101)\n",
    "\n",
    "data <- read.table(\"credit_card_data-headers.txt\", header = TRUE)\n",
    "kernels = c(\"rectangular\", \"triangular\", \"epanechnikov\", \"biweight\", \"triweight\", \"cos\", \"inv\", \"gaussian\", \"optimal\")\n",
    "\n",
    "cat(\"-------------------------------------BEGIN train.kknn method -------------------------------------------\\n\")\n",
    "train_model <- train.kknn(R1~., data, kmax = 200, distance = 2, kernel = kernels, scale = TRUE)\n",
    "train_model\n",
    "plot(train_model)\n",
    "title(\"Kernels using train.kknn\")\n",
    "\n",
    "best_train_kernel <- train_model$best.parameters$kernel\n",
    "best_train_kval <- train_model$best.parameters$k\n",
    "kknn_train_model <- train.kknn(R1~., data, ks=best_train_kval, distance = 2, kernel = best_train_kernel, scale = TRUE)\n",
    "pred <- round(predict(kknn_train_model, data[,1:10]))\n",
    "train_model_accuracy <- sum(pred == data[,11])/length(data[,11])\n",
    "cat(\"\\nBest Kernel is:\",best_train_kernel,\", Best K Value is:\",best_train_kval,\", and the accuracy is:\",train_model_accuracy*100,\"%\\n\")\n",
    "cat(\"\\n-------------------------------------END train.kknn method -------------------------------------------\\n\\n\")\n",
    "cat(\"\\n-------------------------------------BEGIN cv.kknn method -------------------------------------------\\n\")\n",
    "kvals <- c(1:100)\n",
    "model_accuracy <- c()\n",
    "\n",
    "for (i in 1:length(kernels)){\n",
    "  for (j in 1:length(kvals)){\n",
    "    cv_model <- cv.kknn(R1~., data= data, kcv = 10, scale = TRUE, kernel = kernels[i], k = kvals[j])\n",
    "    cv_model <- data.table(cv_model[[1]])\n",
    "    cv_model_accuracy <- sum(round(cv_model$yhat) == data$R1)/length(data$R1)\n",
    "    model_accuracy <- c(model_accuracy, cv_model_accuracy)\n",
    "  }\n",
    "  cat(\"\\nFor\", kernels[i], \"kernel, model accuracy is:\", max(model_accuracy)*100,\"% and corresponding k-value is:\",kvals[which.max(model_accuracy[1:length(kvals)])])\n",
    "\n",
    "}\n",
    "cat(\"\\n-------------------------------------END cv.kknn method -------------------------------------------\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3.1 (b):\n",
    "````\n",
    "Here, I am splitting the data to train, validate and test. I am using 4 different ratios :\n",
    "(50%,25%,25%), (60%,20%,20%),  (70%,15%,15%), and (80%,10%,10%).\n",
    "I am going to run the ksvm model on the split datasets to predict the best model classifier.\n",
    "I am not going to use Rotating method at this point.\n",
    "\n",
    "For the different ratios, I am predicting model accuracies for four different kernels. Iterating over kernels and cost values for each ratio, the best TEST ACCURACY is 90.90909% (for polydot kernel and 80/10/10 split). However this accuracy value is a lot higher compared to the training accuracy (85.27725%). \"Anovadot\" however, seems to be more legit. For a split ratio of 60/20/20, Test Accuracy is 89.39394 %, validation accuracy is 83.07692 % and training accuracy is 92.34694 %.\n",
    "\n",
    "From Problem 2.1 (a) we concluded that rbfdot with C = 10000 had the best model accuracy. However, the accuracy for that parameters on test data was considerably lower at 80.80808%. \n",
    "\n",
    "\n",
    "````\n",
    "#### Documentation on Stackoverflow showing examples to split data frame into train,validation and test datasets. \n",
    "https://stackoverflow.com/questions/36068963/r-how-to-split-a-data-frame-into-training-validation-and-test-sets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th></th><th scope=col>Kernel</th><th scope=col>Cost Value</th><th scope=col>Train Split %</th><th scope=col>Validate Split %</th><th scope=col>Test Split %</th><th scope=col>Train Accuracy %</th><th scope=col>Validate Accuracy %</th><th scope=col>Test Accuracy %</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>16</th><td>polydot   </td><td>    1     </td><td>80        </td><td>10        </td><td>10        </td><td> 85.27725 </td><td>90.76923  </td><td>90.90909  </td></tr>\n",
       "\t<tr><th scope=row>14</th><td>polydot   </td><td>    1     </td><td>60        </td><td>20        </td><td>20        </td><td> 85.96939 </td><td>83.07692  </td><td>90.15152  </td></tr>\n",
       "\t<tr><th scope=row>2</th><td>vanilladot</td><td>    1     </td><td>60        </td><td>20        </td><td>20        </td><td> 85.96939 </td><td>84.61538  </td><td>89.39394  </td></tr>\n",
       "\t<tr><th scope=row>10</th><td>anovadot  </td><td>10000     </td><td>60        </td><td>20        </td><td>20        </td><td> 92.34694 </td><td>83.07692  </td><td>89.39394  </td></tr>\n",
       "\t<tr><th scope=row>15</th><td>polydot   </td><td>    1     </td><td>70        </td><td>15        </td><td>15        </td><td> 85.12035 </td><td>89.79592  </td><td>88.88889  </td></tr>\n",
       "\t<tr><th scope=row>11</th><td>anovadot  </td><td>10000     </td><td>70        </td><td>15        </td><td>15        </td><td> 89.71554 </td><td>90.81633  </td><td>86.86869  </td></tr>\n",
       "\t<tr><th scope=row>13</th><td>polydot   </td><td>    1     </td><td>50        </td><td>25        </td><td>25        </td><td> 86.23853 </td><td>87.11656  </td><td>85.36585  </td></tr>\n",
       "\t<tr><th scope=row>4</th><td>vanilladot</td><td>    1     </td><td>80        </td><td>10        </td><td>10        </td><td> 86.61568 </td><td>86.15385  </td><td>84.84848  </td></tr>\n",
       "\t<tr><th scope=row>1</th><td>vanilladot</td><td>    1     </td><td>50        </td><td>25        </td><td>25        </td><td> 85.32110 </td><td>90.18405  </td><td>84.14634  </td></tr>\n",
       "\t<tr><th scope=row>9</th><td>anovadot  </td><td>10000     </td><td>50        </td><td>25        </td><td>25        </td><td> 94.80122 </td><td>80.36810  </td><td>84.14634  </td></tr>\n",
       "\t<tr><th scope=row>12</th><td>anovadot  </td><td>10000     </td><td>80        </td><td>10        </td><td>10        </td><td> 92.92543 </td><td>81.53846  </td><td>81.81818  </td></tr>\n",
       "\t<tr><th scope=row>3</th><td>vanilladot</td><td>    1     </td><td>70        </td><td>15        </td><td>15        </td><td> 87.52735 </td><td>86.73469  </td><td>80.80808  </td></tr>\n",
       "\t<tr><th scope=row>7</th><td>rbfdot    </td><td>10000     </td><td>70        </td><td>15        </td><td>15        </td><td> 99.34354 </td><td>77.55102  </td><td>80.80808  </td></tr>\n",
       "\t<tr><th scope=row>8</th><td>rbfdot    </td><td>10000     </td><td>80        </td><td>10        </td><td>10        </td><td> 99.42639 </td><td>73.84615  </td><td>78.78788  </td></tr>\n",
       "\t<tr><th scope=row>5</th><td>rbfdot    </td><td>10000     </td><td>50        </td><td>25        </td><td>25        </td><td>100.00000 </td><td>73.61963  </td><td>76.82927  </td></tr>\n",
       "\t<tr><th scope=row>6</th><td>rbfdot    </td><td>10000     </td><td>60        </td><td>20        </td><td>20        </td><td> 99.23469 </td><td>75.38462  </td><td>76.51515  </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|llllllll}\n",
       "  & Kernel & Cost Value & Train Split \\% & Validate Split \\% & Test Split \\% & Train Accuracy \\% & Validate Accuracy \\% & Test Accuracy \\%\\\\\n",
       "\\hline\n",
       "\t16 & polydot    &     1      & 80         & 10         & 10         &  85.27725  & 90.76923   & 90.90909  \\\\\n",
       "\t14 & polydot    &     1      & 60         & 20         & 20         &  85.96939  & 83.07692   & 90.15152  \\\\\n",
       "\t2 & vanilladot &     1      & 60         & 20         & 20         &  85.96939  & 84.61538   & 89.39394  \\\\\n",
       "\t10 & anovadot   & 10000      & 60         & 20         & 20         &  92.34694  & 83.07692   & 89.39394  \\\\\n",
       "\t15 & polydot    &     1      & 70         & 15         & 15         &  85.12035  & 89.79592   & 88.88889  \\\\\n",
       "\t11 & anovadot   & 10000      & 70         & 15         & 15         &  89.71554  & 90.81633   & 86.86869  \\\\\n",
       "\t13 & polydot    &     1      & 50         & 25         & 25         &  86.23853  & 87.11656   & 85.36585  \\\\\n",
       "\t4 & vanilladot &     1      & 80         & 10         & 10         &  86.61568  & 86.15385   & 84.84848  \\\\\n",
       "\t1 & vanilladot &     1      & 50         & 25         & 25         &  85.32110  & 90.18405   & 84.14634  \\\\\n",
       "\t9 & anovadot   & 10000      & 50         & 25         & 25         &  94.80122  & 80.36810   & 84.14634  \\\\\n",
       "\t12 & anovadot   & 10000      & 80         & 10         & 10         &  92.92543  & 81.53846   & 81.81818  \\\\\n",
       "\t3 & vanilladot &     1      & 70         & 15         & 15         &  87.52735  & 86.73469   & 80.80808  \\\\\n",
       "\t7 & rbfdot     & 10000      & 70         & 15         & 15         &  99.34354  & 77.55102   & 80.80808  \\\\\n",
       "\t8 & rbfdot     & 10000      & 80         & 10         & 10         &  99.42639  & 73.84615   & 78.78788  \\\\\n",
       "\t5 & rbfdot     & 10000      & 50         & 25         & 25         & 100.00000  & 73.61963   & 76.82927  \\\\\n",
       "\t6 & rbfdot     & 10000      & 60         & 20         & 20         &  99.23469  & 75.38462   & 76.51515  \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| <!--/--> | Kernel | Cost Value | Train Split % | Validate Split % | Test Split % | Train Accuracy % | Validate Accuracy % | Test Accuracy % | \n",
       "|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n",
       "| 16 | polydot    |     1      | 80         | 10         | 10         |  85.27725  | 90.76923   | 90.90909   | \n",
       "| 14 | polydot    |     1      | 60         | 20         | 20         |  85.96939  | 83.07692   | 90.15152   | \n",
       "| 2 | vanilladot |     1      | 60         | 20         | 20         |  85.96939  | 84.61538   | 89.39394   | \n",
       "| 10 | anovadot   | 10000      | 60         | 20         | 20         |  92.34694  | 83.07692   | 89.39394   | \n",
       "| 15 | polydot    |     1      | 70         | 15         | 15         |  85.12035  | 89.79592   | 88.88889   | \n",
       "| 11 | anovadot   | 10000      | 70         | 15         | 15         |  89.71554  | 90.81633   | 86.86869   | \n",
       "| 13 | polydot    |     1      | 50         | 25         | 25         |  86.23853  | 87.11656   | 85.36585   | \n",
       "| 4 | vanilladot |     1      | 80         | 10         | 10         |  86.61568  | 86.15385   | 84.84848   | \n",
       "| 1 | vanilladot |     1      | 50         | 25         | 25         |  85.32110  | 90.18405   | 84.14634   | \n",
       "| 9 | anovadot   | 10000      | 50         | 25         | 25         |  94.80122  | 80.36810   | 84.14634   | \n",
       "| 12 | anovadot   | 10000      | 80         | 10         | 10         |  92.92543  | 81.53846   | 81.81818   | \n",
       "| 3 | vanilladot |     1      | 70         | 15         | 15         |  87.52735  | 86.73469   | 80.80808   | \n",
       "| 7 | rbfdot     | 10000      | 70         | 15         | 15         |  99.34354  | 77.55102   | 80.80808   | \n",
       "| 8 | rbfdot     | 10000      | 80         | 10         | 10         |  99.42639  | 73.84615   | 78.78788   | \n",
       "| 5 | rbfdot     | 10000      | 50         | 25         | 25         | 100.00000  | 73.61963   | 76.82927   | \n",
       "| 6 | rbfdot     | 10000      | 60         | 20         | 20         |  99.23469  | 75.38462   | 76.51515   | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "   Kernel     Cost Value Train Split % Validate Split % Test Split %\n",
       "16 polydot        1      80            10               10          \n",
       "14 polydot        1      60            20               20          \n",
       "2  vanilladot     1      60            20               20          \n",
       "10 anovadot   10000      60            20               20          \n",
       "15 polydot        1      70            15               15          \n",
       "11 anovadot   10000      70            15               15          \n",
       "13 polydot        1      50            25               25          \n",
       "4  vanilladot     1      80            10               10          \n",
       "1  vanilladot     1      50            25               25          \n",
       "9  anovadot   10000      50            25               25          \n",
       "12 anovadot   10000      80            10               10          \n",
       "3  vanilladot     1      70            15               15          \n",
       "7  rbfdot     10000      70            15               15          \n",
       "8  rbfdot     10000      80            10               10          \n",
       "5  rbfdot     10000      50            25               25          \n",
       "6  rbfdot     10000      60            20               20          \n",
       "   Train Accuracy % Validate Accuracy % Test Accuracy %\n",
       "16  85.27725        90.76923            90.90909       \n",
       "14  85.96939        83.07692            90.15152       \n",
       "2   85.96939        84.61538            89.39394       \n",
       "10  92.34694        83.07692            89.39394       \n",
       "15  85.12035        89.79592            88.88889       \n",
       "11  89.71554        90.81633            86.86869       \n",
       "13  86.23853        87.11656            85.36585       \n",
       "4   86.61568        86.15385            84.84848       \n",
       "1   85.32110        90.18405            84.14634       \n",
       "9   94.80122        80.36810            84.14634       \n",
       "12  92.92543        81.53846            81.81818       \n",
       "3   87.52735        86.73469            80.80808       \n",
       "7   99.34354        77.55102            80.80808       \n",
       "8   99.42639        73.84615            78.78788       \n",
       "5  100.00000        73.61963            76.82927       \n",
       "6   99.23469        75.38462            76.51515       "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "library(kernlab)\n",
    "library(e1071)\n",
    "library(caTools)\n",
    "require(caTools)\n",
    "set.seed(101)\n",
    "\n",
    "data <- read.table(\"credit_card_data-headers.txt\", header = TRUE)\n",
    "kernel_list = c(\"vanilladot\", \"rbfdot\", \"anovadot\", \"polydot\")\n",
    "\n",
    "w <- c(0.5,0.25,0.25)\n",
    "x <- c(0.6,0.2,0.2)\n",
    "y <- c(0.7,0.15,0.15)\n",
    "z <- c(0.8,0.1,0.1)\n",
    "m <- list(w,x,y,z)\n",
    "\n",
    "cost_value <- c()\n",
    "kernel_val <- c()\n",
    "train_frac_list <- c()\n",
    "validate_frac_list <- c()\n",
    "test_frac_list <- c()\n",
    "Train_Accuracy  <- c()\n",
    "Validation_Accuracy  <- c()\n",
    "Test_Accuracy  <- c()\n",
    "\n",
    "for (k1 in 1:length(kernel_list)){\n",
    "    for (p in 1:length(m)){\n",
    "        fractionTraining <- m[[p]][1]\n",
    "        fractionValidation <- m[[p]][2]\n",
    "        fractionTest <- m[[p]][3]\n",
    "\n",
    "        sampleSizeTraining <- floor(fractionTraining * nrow(data))\n",
    "        sampleSizeValidation <- floor(fractionValidation * nrow(data))\n",
    "        sampleSizeTest <- floor(fractionTest * nrow(data))\n",
    "\n",
    "        indicesTraining <- sort(sample(seq_len(nrow(data)), size=sampleSizeTraining))\n",
    "        indicesNotTraining <- setdiff(seq_len(nrow(data)), indicesTraining)\n",
    "        indicesValidation <- sort(sample(indicesNotTraining, size=sampleSizeValidation))\n",
    "        indicesTest <- setdiff(indicesNotTraining, indicesValidation)\n",
    "\n",
    "        dfTraining   <- data[indicesTraining, ]\n",
    "        x_dfTraining <- dfTraining[,1:10]\n",
    "        y_dfTraining <- dfTraining[,11]\n",
    "        dfValidation <- data[indicesValidation, ]\n",
    "        x_dfValidation <- dfValidation[,1:10]\n",
    "        y_dfValidation <- dfValidation[,11]\n",
    "        dfTest       <- data[indicesTest, ]\n",
    "        x_dfTest <- dfTest[,1:10]\n",
    "        y_dfTest <- dfTest[,11]\n",
    "\n",
    "        cost_values = c(1, 10, 100, 1000, 10000, 0.1, 0.01, 0.001)\n",
    "        train_accuracy <- c()\n",
    "        prediction_train <- c()\n",
    "        train_model <- c()\n",
    "\n",
    "        for (i in 1:length(cost_values)){\n",
    "          ksvm_model <- ksvm(as.matrix(x_dfTraining), as.factor(y_dfTraining), type = \"C-svc\", kernel = kernel_list[k1], C = cost_values[i], scaled = TRUE)\n",
    "          a <- colSums(ksvm_model@xmatrix[[1]]*ksvm_model@coef[[1]])\n",
    "          a0 <- ksvm_model@b\n",
    "          accuracy <- sum(predict(ksvm_model,x_dfTraining) == y_dfTraining)/length(y_dfTraining)\n",
    "          train_accuracy <- c(train_accuracy,accuracy)\n",
    "          train_model <- c(train_model, ksvm_model)\n",
    "          prediction_train <- c(prediction_train, predict(ksvm_model,x_dfTraining))\n",
    "        }\n",
    "\n",
    "        model <- train_model[which.max(train_accuracy[1:length(cost_values)])]\n",
    "        C_val = cost_values[which.max(train_accuracy[1:length(cost_values)])]\n",
    "        train_accuracy <- max(train_accuracy[1:length(cost_values)])\n",
    "\n",
    "        validation_predict <- predict(model[[1]],x_dfValidation)\n",
    "        validation_accuracy <- sum(validation_predict == y_dfValidation)/length(y_dfValidation)\n",
    "\n",
    "        test_predict <- predict(model[[1]],x_dfTest)\n",
    "        test_accuracy <- sum(test_predict == y_dfTest)/length(y_dfTest)\n",
    "\n",
    "        Train_Accuracy <- c(Train_Accuracy, train_accuracy) \n",
    "        Validation_Accuracy <- c(Validation_Accuracy, validation_accuracy) \n",
    "        Test_Accuracy <- c(Test_Accuracy, test_accuracy)\n",
    "        cost_value <- c(cost_value, C_val)\n",
    "        train_frac_list <- c(train_frac_list, fractionTraining*100)\n",
    "        validate_frac_list <- c(validate_frac_list, fractionValidation*100)\n",
    "        test_frac_list <- c(test_frac_list, fractionTest*100)\n",
    "        kernel_val <- c(kernel_val, kernel_list[k1])\n",
    "    }\n",
    "}\n",
    "df <- data.frame(kernel_val, cost_value, train_frac_list, validate_frac_list, test_frac_list, Train_Accuracy*100, Validation_Accuracy*100, Test_Accuracy*100)\n",
    "names(df) <- c(\"Kernel\",\"Cost Value\",\"Train Split %\",\"Validate Split %\",\"Test Split %\",\"Train Accuracy %\",\"Validate Accuracy %\",\"Test Accuracy %\")\n",
    "df[order(df[\"Test Accuracy %\"], decreasing = TRUE), ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
